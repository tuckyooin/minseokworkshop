import os
import re
import math
import time
import requests
import pandas as pd
import streamlit as st
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus

# =========================
# Secrets / Keys
# =========================
YOUTUBE_API_KEY  = st.secrets.get("YOUTUBE_API_KEY", os.getenv("YOUTUBE_API_KEY", ""))
PEXELS_API_KEY   = st.secrets.get("PEXELS_API_KEY",  os.getenv("PEXELS_API_KEY",  ""))
REDDIT_UA        = st.secrets.get("REDDIT_USER_AGENT", os.getenv("REDDIT_USER_AGENT", "ShortsFinder/1.0 by minsuk"))

# =========================
# Endpoints / Const
# =========================
YOUTUBE_SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
YOUTUBE_VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
REDDIT_SEARCH_URL  = "https://www.reddit.com/search.json"
PEXELS_SEARCH_URL  = "https://api.pexels.com/videos/search"
PEXELS_POPULAR_URL = "https://api.pexels.com/videos/popular"
REQUEST_TIMEOUT    = 12

# =========================
# Helpers
# =========================
def http_get(url, params=None, headers=None, timeout=REQUEST_TIMEOUT, max_retries=2, backoff=0.8):
    last_err = None
    for i in range(max_retries + 1):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=timeout)
            if r.status_code == 200:
                try:
                    return r.json()
                except Exception:
                    last_err = "Invalid JSON response"
            else:
                last_err = f"HTTP {r.status_code}: {r.text[:200]}"
        except Exception as e:
            last_err = str(e)
        time.sleep(backoff * (2 ** i))
    raise RuntimeError(last_err or "Unknown network error")

def fmt_int(n):
    try:
        n = int(n)
    except:
        return n
    if n >= 1_000_000_000: return f"{n/1_000_000_000:.1f}B"
    if n >= 1_000_000:     return f"{n/1_000_000:.1f}M"
    if n >= 1_000:         return f"{n/1_000:.1f}K"
    return str(n)

def parse_iso8601_duration(s: str) -> int:
    if not s: return 0
    m = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", s)
    if not m: return 0
    h = int(m.group(1) or 0)
    mi = int(m.group(2) or 0)
    se = int(m.group(3) or 0)
    return h*3600 + mi*60 + se

def published_after_from_option(opt: str) -> str | None:
    now = datetime.now(timezone.utc)
    mapping = {
        "ì „ì²´": None,
        "ìµœê·¼ 24ì‹œê°„": now - timedelta(days=1),
        "ìµœê·¼ 7ì¼":    now - timedelta(days=7),
        "ìµœê·¼ 30ì¼":   now - timedelta(days=30),
        "ìµœê·¼ 1ë…„":    now - timedelta(days=365),
    }
    dt = mapping.get(opt)
    return dt.isoformat().replace("+00:00", "Z") if dt else None

def pexels_search_link(query: str) -> str:
    return f"https://www.pexels.com/search/videos/{quote_plus(query)}/"

# =========================
# Session
# =========================
def init_state():
    if "page" not in st.session_state:
        st.session_state.page = 1
    if "page_size" not in st.session_state:
        st.session_state.page_size = 10
    if "bookmarks" not in st.session_state:
        st.session_state.bookmarks = []  # list of dict rows
    if "search_history" not in st.session_state:
        st.session_state.search_history = []  # ìµœê·¼ ê²€ìƒ‰ì–´ 10ê°œ
    if "yt_sort" not in st.session_state:
        st.session_state.yt_sort = "ì¡°íšŒìˆ˜ìˆœ"
    if "left_cols" not in st.session_state:
        st.session_state.left_cols = 2
    if "right_cols" not in st.session_state:
        st.session_state.right_cols = 2
    if "accent" not in st.session_state:
        st.session_state.accent = "ê¸°ë³¸"


def page_controls(total_count: int, where: str):
    total_pages = max(1, math.ceil(total_count / st.session_state.page_size))
    cols = st.columns([1,1,2,2,1,1])
    with cols[0]:
        if st.button("â®ï¸ ì²˜ìŒ", key=f"{where}_first"):
            st.session_state.page = 1
    with cols[1]:
        if st.button("â—€ï¸ ì´ì „", key=f"{where}_prev"):
            st.session_state.page = max(1, st.session_state.page - 1)
    with cols[2]:
        st.session_state.page_size = st.selectbox(
            "í•œ í˜ì´ì§€ ë³´ê¸°",
            options=[10,20,50],
            index=[10,20,50].index(st.session_state.page_size) if st.session_state.page_size in (10,20,50) else 0,
            key=f"{where}_pagesize"
        )
    with cols[3]:
        st.session_state.page = st.number_input(
            f"í˜ì´ì§€ (ì´ {total_pages}p)",
            min_value=1, max_value=total_pages, value=min(st.session_state.page, total_pages),
            step=1, key=f"{where}_pnum"
        )
    with cols[4]:
        if st.button("ë‹¤ìŒ â–¶ï¸", key=f"{where}_next"):
            st.session_state.page = min(total_pages, st.session_state.page + 1)
    with cols[5]:
        if st.button("ë§ˆì§€ë§‰ â­ï¸", key=f"{where}_last"):
            st.session_state.page = total_pages


def slice_df_for_page(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    ps = st.session_state.page_size
    pg = st.session_state.page
    start = (pg - 1) * ps
    end = start + ps
    return df.iloc[start:end]


def render_cards(df: pd.DataFrame, *, cols: int, subtitles: list[str], bookmark_key_prefix: str):
    if df is None or df.empty:
        st.info("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    rows = math.ceil(len(df) / cols)
    for r in range(rows):
        ccols = st.columns(cols)
        for i in range(cols):
            idx = r*cols + i
            if idx >= len(df): break
            row = df.iloc[idx].to_dict()
            with ccols[i]:
                if row.get("thumbnail"):
                    st.image(row["thumbnail"], use_column_width=True)
                title = row.get("title", "Untitled")
                url = row.get("url", "#")
                st.markdown(f"**[{title}]({url})**", unsafe_allow_html=True)

                chips = []
                for key in subtitles:
                    val = row.get(key)
                    if val in (None, "", 0):
                        continue
                    if key == "author":           chips.append(f"ì œì‘ì {val}")
                    elif key == "views":         chips.append(f"ì¡°íšŒìˆ˜ {fmt_int(val)}")
                    elif key == "duration":      chips.append(f"ê¸¸ì´ {val}s")
                    elif key == "durationText":  chips.append(f"ê¸¸ì´ {val}")
                    elif key == "publishedAt":   chips.append(f"ê²Œì‹œ {str(val)[:10]}")
                    elif key == "license":       chips.append(f"ë¼ì´ì„ ìŠ¤ {val}")
                if chips:
                    st.caption(" Â· ".join(chips))

                # ë¶ë§ˆí¬
                book_id = f"{bookmark_key_prefix}_{idx}_{hash(url)%10_000_000}"
                if st.button("â­ ë¶ë§ˆí¬", key=book_id):
                    st.session_state.bookmarks.append(row)
                    st.toast("ë¶ë§ˆí¬ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤", icon="â­")


# =========================
# Data Sources (YouTube / Pexels / Reddit[ì˜µì…˜])
# =========================
@st.cache_data(show_spinner=False, ttl=900)
def search_youtube(query: str, *, fetch: int, cc_only: bool, upload_window: str,
                   include_channels: list[str], exclude_channels: list[str],
                   include_channel_ids: list[str], exclude_channel_ids: list[str],
                   include_words: list[str], exclude_words: list[str],
                   region_code: str | None, relevance_lang: str | None,
                   safe_mode: str, seed_order_viewcount: bool,
                   duration_param: str,
                   min_seconds: int | None, max_seconds: int | None):
    if not YOUTUBE_API_KEY:
        raise RuntimeError("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤. secrets.tomlì— YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    params = {
        "part": "snippet",
        "q": query,
        "maxResults": min(fetch, 50),
        "type": "video",
        "key": YOUTUBE_API_KEY,
    }
    if cc_only:
        params["videoLicense"] = "creativeCommon"
    if seed_order_viewcount:
        params["order"] = "viewCount"
    else:
        params["order"] = "date"
    if region_code:
        params["regionCode"] = region_code
    if relevance_lang:
        params["relevanceLanguage"] = relevance_lang
    if safe_mode in ("none","moderate","strict"):
        params["safeSearch"] = safe_mode
    if duration_param in ("any","short","medium","long") and duration_param != "any":
        params["videoDuration"] = duration_param

    pub_after = published_after_from_option(upload_window)
    if pub_after:
        params["publishedAfter"] = pub_after

    sjson = http_get(YOUTUBE_SEARCH_URL, params=params)
    items = sjson.get("items", [])
    video_ids = [it.get("id", {}).get("videoId") for it in items if it.get("id", {}).get("videoId")]
    if not video_ids:
        return []

    params2 = {
        "part": "snippet,statistics,contentDetails",
        "id": ",".join(video_ids),
        "key": YOUTUBE_API_KEY
    }
    vjson = http_get(YOUTUBE_VIDEOS_URL, params=params2)

    out = []
    for v in vjson.get("items", []):
        vid = v["id"]
        sn  = v.get("snippet", {})
        stt = v.get("statistics", {})
        cd  = v.get("contentDetails", {})
        thumbs = sn.get("thumbnails", {})
        thumb = (thumbs.get("medium") or thumbs.get("high") or thumbs.get("default") or {}).get("url")

        channel_title = sn.get("channelTitle", "")
        channel_id    = sn.get("channelId", "")
        seconds = parse_iso8601_duration(cd.get("duration"))
        is_shorts = seconds <= 60 if seconds else False

        title_l = (sn.get("title") or "").lower()

        # í¬í•¨/ì œì™¸ í•„í„°ë§ (ì œëª© í‚¤ì›Œë“œ)
        if include_words and not all(w.lower() in title_l for w in include_words):
            continue
        if exclude_words and any(w.lower() in title_l for w in exclude_words):
            continue
        if include_channels and channel_title not in include_channels:
            continue
        if exclude_channels and channel_title in exclude_channels:
            continue
        if include_channel_ids and channel_id not in include_channel_ids:
            continue
        if exclude_channel_ids and channel_id in exclude_channel_ids:
            continue
        if (min_seconds is not None and seconds < min_seconds) or (max_seconds is not None and seconds > max_seconds):
            continue

        out.append({
            "platform": "YouTube",
            "title": sn.get("title"),
            "author": channel_title,
            "views": int(stt.get("viewCount", 0)) if stt.get("viewCount") else None,
            "url": f"https://www.youtube.com/watch?v={vid}",
            "thumbnail": thumb,
            "publishedAt": sn.get("publishedAt"),
            "durationSec": seconds,
            "durationText": str(timedelta(seconds=seconds)) if seconds else "",
            "isShorts": is_shorts,
            "license": "CC" if cc_only else "Standard/Unknown",
        })

    # ê²°ê³¼ ì •ë ¬(í‘œì‹œìš©): ìƒë‹¨ í† ê¸€ ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬
    # ì¡°íšŒìˆ˜ìˆœ â†’ views desc, ìµœì‹ ìˆœ â†’ publishedAt desc
    if st.session_state.yt_sort == "ì¡°íšŒìˆ˜ìˆœ":
        out.sort(key=lambda x: (x["views"] or -1), reverse=True)
    else:
        out.sort(key=lambda x: x.get("publishedAt", ""), reverse=True)
    return out


@st.cache_data(show_spinner=False, ttl=900)
def search_pexels(query: str, *, fetch: int, mode="search"):
    if not PEXELS_API_KEY:
        return []
    headers = {"Authorization": PEXELS_API_KEY}
    url = PEXELS_POPULAR_URL if mode == "popular" else PEXELS_SEARCH_URL
    params = {"per_page": min(fetch, 80)}
    if mode == "search":
        params["query"] = query
    data = http_get(url, params=params, headers=headers)
    out = []
    for v in data.get("videos", []):
        out.append({
            "platform": "Pexels",
            "title": f"Pexels Video by {v.get('user',{}).get('name','Creator')}",
            "author": v.get("user",{}).get("name"),
            "views": None,
            "url": v.get("url"),
            "thumbnail": v.get("image"),
            "duration": v.get("duration"),
        })
    return out


@st.cache_data(show_spinner=False, ttl=900)
def search_reddit(query: str, *, fetch: int, sort="relevance", time_filter="all", safe_mode: bool = True):
    headers = {"User-Agent": REDDIT_UA}
    params = {"q": query, "limit": min(fetch, 100), "sort": sort, "t": time_filter, "type": "link"}
    data = http_get(REDDIT_SEARCH_URL, params=params, headers=headers)
    out = []
    for child in data.get("data", {}).get("children", []):
        d = child.get("data", {})
        if safe_mode and d.get("over_18"):
            continue
        thumb = None
        if d.get("thumbnail") and str(d["thumbnail"]).startswith("http"):
            thumb = d["thumbnail"]
        elif "preview" in d and d["preview"].get("images"):
            thumb = d["preview"]["images"][0]["source"].get("url")
        link = d.get("url_overridden_by_dest") or d.get("url")
        out.append({
            "platform": "Reddit",
            "title": d.get("title"),
            "author": d.get("author"),
            "score": d.get("score"),
            "comments": d.get("num_comments"),
            "url": link,
            "thumbnail": thumb,
            "permalink": "https://www.reddit.com" + d.get("permalink", ""),
        })
    return out


# =========================
# UI
# =========================
st.set_page_config(page_title="ë¯¼ì„ì´ì˜ ì‘ì—…ì‹¤", page_icon="ğŸ› ï¸", layout="wide")
st.title("ğŸ› ï¸ ë¯¼ì„ì´ì˜ ì‘ì—…ì‹¤")
st.caption("ì›ë³¸ ë§í¬ë¡œë§Œ ì´ë™í•˜ëŠ” ì•ˆì „í•œ ê°œì¸ìš© ê²€ìƒ‰ ë„êµ¬ (ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì—†ìŒ)")

init_state()

# Accent Theme (CSS)
accent_map = {
    "ê¸°ë³¸": "#00A389",
    "ë¸”ë£¨": "#2F80ED",
    "ê·¸ë¦°": "#27AE60",
    "í•‘í¬": "#EB5757",
    "ë³´ë¼": "#A259FF",
}
accent = accent_map.get(st.session_state.accent, "#00A389")

st.markdown(f"""
<style>
:root {{ --accent: {accent}; }}
.block-container {{ padding-top: 1.0rem; }}
.v-sep {{ border-left: 1px solid #ddd; height: 100%; opacity: 0.4; }}
a, .st-emotion-cache-1kyxreq a {{ color: var(--accent) !important; }}
.stButton>button, .stDownloadButton button {{ border-radius: 999px; border: 1px solid var(--accent); color: var(--accent); background: transparent; }}
.stButton>button:hover, .stDownloadButton button:hover {{ background: var(--accent); color: white; }}
</style>
""", unsafe_allow_html=True)

# ===== ì‚¬ì´ë“œë°” (í•œê¸€ UI) =====
with st.sidebar:
    st.header("ê²€ìƒ‰ ì˜µì…˜")

    # ìµœê·¼ ê²€ìƒ‰
    if st.session_state.search_history:
        st.caption("ìµœê·¼ ê²€ìƒ‰")
        hist_cols = st.columns(3)
        for i, qv in enumerate(st.session_state.search_history[-9:][::-1]):
            with hist_cols[i % 3]:
                if st.button(qv, key=f"hist_{i}"):
                    st.session_state["_prefill_query"] = qv

    # ê²€ìƒ‰ì–´
    q_default = st.session_state.get("_prefill_query", "prank")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value=q_default)
    st.session_state["_prefill_query"] = query

    st.markdown("---")

    # í™”ë©´ ë°°ì¹˜
    layout_mode = st.selectbox("í™”ë©´ ë°°ì¹˜", ["ê°€ë¡œ 2íŒ¨ë„", "ìƒí•˜ ìŠ¤íƒ"], index=0)

    # ë¯¼ê°ë„(YouTube safeSearch)
    safe_label = st.selectbox("ë¯¼ê°ë„ í•„í„°(YouTube)", ["ê°•í•¨", "ë³´í†µ", "í•´ì œ"], index=1)
    yt_safe = {"ê°•í•¨":"strict", "ë³´í†µ":"moderate", "í•´ì œ":"none"}[safe_label]

    # YouTube ì„¤ì • (ì •ë ¬ì€ ìƒë‹¨ íˆ´ë°”ë¡œ ì´ë™)
    st.subheader("YouTube ì„¤ì •")
    yt_cc   = st.checkbox("í¬ë¦¬ì—ì´í‹°ë¸Œ ì»¤ë¨¼ì¦ˆ(CC)ë§Œ", value=False)
    yt_upload_window = st.selectbox("ì—…ë¡œë“œ ì‹œì ", ["ì „ì²´", "ìµœê·¼ 24ì‹œê°„", "ìµœê·¼ 7ì¼", "ìµœê·¼ 30ì¼", "ìµœê·¼ 1ë…„"], index=0)

    # ì§€ì—­/ì–¸ì–´ (í•œêµ­ì–´ ë¼ë²¨ â†’ ì½”ë“œ ë§¤í•‘)
    region_label = st.selectbox("ì§€ì—­", ["ìë™", "í•œêµ­(KR)", "ë¯¸êµ­(US)", "ì¼ë³¸(JP)", "ì˜êµ­(GB)", "ë…ì¼(DE)", "í”„ë‘ìŠ¤(FR)", "ì¸ë„(IN)", "ì¸ë„ë„¤ì‹œì•„(ID)", "ë¸Œë¼ì§ˆ(BR)", "ë©•ì‹œì½”(MX)"] , index=1)
    region_map = {"ìë™":"", "í•œêµ­(KR)":"KR", "ë¯¸êµ­(US)":"US", "ì¼ë³¸(JP)":"JP", "ì˜êµ­(GB)":"GB", "ë…ì¼(DE)":"DE", "í”„ë‘ìŠ¤(FR)":"FR", "ì¸ë„(IN)":"IN", "ì¸ë„ë„¤ì‹œì•„(ID)":"ID", "ë¸Œë¼ì§ˆ(BR)":"BR", "ë©•ì‹œì½”(MX)":"MX"}
    region_code = region_map[region_label]

    lang_label = st.selectbox("ì–¸ì–´", ["ìë™", "í•œêµ­ì–´(ko)", "ì˜ì–´(en)", "ì¼ë³¸ì–´(ja)", "ìŠ¤í˜ì¸ì–´(es)", "í”„ë‘ìŠ¤ì–´(fr)", "ë…ì¼ì–´(de)", "ì¸ë„ë„¤ì‹œì•„ì–´(id)", "í¬ë¥´íˆ¬ê°ˆì–´(pt)", "íŒë””ì–´(hi)"] , index=1)
    lang_map = {"ìë™":"", "í•œêµ­ì–´(ko)":"ko", "ì˜ì–´(en)":"en", "ì¼ë³¸ì–´(ja)":"ja", "ìŠ¤í˜ì¸ì–´(es)":"es", "í”„ë‘ìŠ¤ì–´(fr)":"fr", "ë…ì¼ì–´(de)":"de", "ì¸ë„ë„¤ì‹œì•„ì–´(id)":"id", "í¬ë¥´íˆ¬ê°ˆì–´(pt)":"pt", "íŒë””ì–´(hi)":"hi"}
    relevance_lang = lang_map[lang_label]

    # ê¸¸ì´ í•„í„° (ë¼ë²¨â†’API ê°’)
    ylen_label = st.selectbox("ê¸¸ì´ í•„í„°", ["ì „ì²´", "ì§§ìŒ(<4ë¶„)", "ì¤‘ê°„(4~20ë¶„)", "ê¹€(>20ë¶„)"], index=0)
    ylen_map = {"ì „ì²´":"any", "ì§§ìŒ(<4ë¶„)":"short", "ì¤‘ê°„(4~20ë¶„)":"medium", "ê¹€(>20ë¶„)":"long"}
    duration_param = ylen_map[ylen_label]

    # ì´ˆ ë‹¨ìœ„ ê¸¸ì´ ë²”ìœ„
    c1, c2 = st.columns(2)
    with c1:
        min_sec = st.number_input("ìµœì†Œ ê¸¸ì´(ì´ˆ)", min_value=0, max_value=86400, value=0, step=5)
        min_seconds = None if min_sec == 0 else int(min_sec)
    with c2:
        max_sec = st.number_input("ìµœëŒ€ ê¸¸ì´(ì´ˆ)", min_value=0, max_value=86400, value=0, step=5)
        max_seconds = None if max_sec == 0 else int(max_sec)

    # ì±„ë„/í‚¤ì›Œë“œ í•„í„°
    inc = st.text_input("í¬í•¨ ì±„ë„ëª…(ì‰¼í‘œ)", value="")
    exc = st.text_input("ì œì™¸ ì±„ë„ëª…(ì‰¼í‘œ)", value="")
    inc_ids = st.text_input("í¬í•¨ ì±„ë„ID(ì‰¼í‘œ)", value="")
    exc_ids = st.text_input("ì œì™¸ ì±„ë„ID(ì‰¼í‘œ)", value="")
    include_channels = [s.strip() for s in inc.split(",") if s.strip()]
    exclude_channels = [s.strip() for s in exc.split(",") if s.strip()]
    include_channel_ids = [s.strip() for s in inc_ids.split(",") if s.strip()]
    exclude_channel_ids = [s.strip() for s in exc_ids.split(",") if s.strip()]

    inc_words = st.text_input("ì œëª©ì— ë°˜ë“œì‹œ í¬í•¨(ì‰¼í‘œ)", value="")
    exc_words = st.text_input("ì œëª©ì— í¬í•¨ë˜ë©´ ì œì™¸(ì‰¼í‘œ)", value="")
    include_words = [s.strip() for s in inc_words.split(",") if s.strip()]
    exclude_words = [s.strip() for s in exc_words.split(",") if s.strip()]

    st.markdown("---")

    # í˜ì´ì§€ í¬ê¸° (ì¢Œ/ìš° ë™ì¼)
    st.subheader("í•œ í˜ì´ì§€ ë³´ê¸°")
    st.session_state.page_size = st.selectbox("í‘œì‹œ ê°œìˆ˜", [10,20,50], index=0)

    st.markdown("---")

    # Pexels ì„¤ì • (ì˜¤ë¥¸ìª½ íŒ¨ë„)
    st.subheader("Pexels ì„¤ì •")
    px_mode_label = st.selectbox("ê²°ê³¼ ìœ í˜•", ["ê²€ìƒ‰ ê²°ê³¼", "ì¸ê¸° ë™ì˜ìƒ"], index=0)
    px_mode = "search" if px_mode_label == "ê²€ìƒ‰ ê²°ê³¼" else "popular"

    st.markdown("---")

    # í…Œë§ˆ ìƒ‰ìƒ ì„ íƒ (ë§í¬/ë²„íŠ¼ í¬ì¸íŠ¸)
    st.subheader("í¬ì¸íŠ¸ ìƒ‰ìƒ")
    st.session_state.accent = st.selectbox("ìƒ‰ìƒ", ["ê¸°ë³¸","ë¸”ë£¨","ê·¸ë¦°","í•‘í¬","ë³´ë¼"], index=["ê¸°ë³¸","ë¸”ë£¨","ê·¸ë¦°","í•‘í¬","ë³´ë¼"].index(st.session_state.accent))

    st.markdown("---")
    run = st.button("ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True)

# ===== ìƒë‹¨ íˆ´ë°”(ì •ë ¬/ë°°ì¹˜) =====
with st.container():
    tb1, tb2, tb3, tb4 = st.columns([1.2,1,1,1])
    with tb1:
        st.session_state.yt_sort = st.radio("YouTube ì •ë ¬", ["ì¡°íšŒìˆ˜ìˆœ","ìµœì‹ ìˆœ"], horizontal=True, index=["ì¡°íšŒìˆ˜ìˆœ","ìµœì‹ ìˆœ"].index(st.session_state.yt_sort))
    with tb2:
        st.session_state.left_cols = st.select_slider("YouTube ì—´ìˆ˜", options=[2,3], value=st.session_state.left_cols)
    with tb3:
        st.session_state.right_cols = st.select_slider("Pexels ì—´ìˆ˜", options=[2,3], value=st.session_state.right_cols)
    with tb4:
        st.write("")
        st.caption("ì •ë ¬ê³¼ ì—´ìˆ˜ëŠ” ìƒë‹¨ì—ì„œ ë¹ ë¥´ê²Œ ë°”ê¿”ìš” âœ¨")

# =========================
# ê²€ìƒ‰ ì‹¤í–‰
# =========================
yt_df_all = pd.DataFrame()
px_df_all = pd.DataFrame()

if run and query:
    # ìµœê·¼ ê²€ìƒ‰ì–´ ì €ì¥ (10ê°œ ìœ ì§€)
    if query not in st.session_state.search_history:
        st.session_state.search_history.append(query)
    if len(st.session_state.search_history) > 10:
        st.session_state.search_history = st.session_state.search_history[-10:]

    with st.spinner("ê²€ìƒ‰ ì¤‘â€¦"):
        # YouTube
        try:
            yt_all = search_youtube(
                query,
                fetch=max(50, st.session_state.page_size),
                cc_only=yt_cc,
                upload_window=yt_upload_window,
                include_channels=include_channels,
                exclude_channels=exclude_channels,
                include_channel_ids=include_channel_ids,
                exclude_channel_ids=exclude_channel_ids,
                include_words=include_words,
                exclude_words=exclude_words,
                region_code=(region_code or None),
                relevance_lang=(relevance_lang or None),
                safe_mode=yt_safe,
                seed_order_viewcount=(st.session_state.yt_sort == "ì¡°íšŒìˆ˜ìˆœ"),
                duration_param=duration_param,
                min_seconds=min_seconds,
                max_seconds=max_seconds,
            )
            yt_df_all = pd.DataFrame(yt_all)
        except Exception as e:
            st.error(f"YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        # Pexels (ì˜¤ë¥¸ìª½ íŒ¨ë„)
        try:
            if PEXELS_API_KEY:
                px_all = search_pexels(query, fetch=max(50, st.session_state.page_size), mode=px_mode)
                px_df_all = pd.DataFrame(px_all)
            else:
                px_df_all = pd.DataFrame()
        except Exception as e:
            st.error(f"Pexels ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    # í˜ì´ì§€ë„¤ì´ì…˜ (ìƒë‹¨)
    total_rows = max(len(yt_df_all), len(px_df_all))
    page_controls(total_rows, where="top")

    yt_df_page = slice_df_for_page(yt_df_all) if not yt_df_all.empty else yt_df_all
    px_df_page = slice_df_for_page(px_df_all) if not px_df_all.empty else px_df_all

    # ===== ë ˆì´ì•„ì›ƒ ë Œë”ë§ =====
    def render_two_panels():
        left, sep, right = st.columns([1, 0.03, 1], gap="large")
        with left:
            st.subheader("ğŸ¬ YouTube")
            if yt_df_page.empty:
                st.info("YouTube ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                tab_all, tab_shorts, tab_video = st.tabs(["ì „ì²´","ì‡¼ì¸ ","ì˜ìƒ"])
                with tab_all:
                    render_cards(yt_df_page, cols=st.session_state.left_cols,
                                 subtitles=["author","views","durationText","publishedAt","license"],
                                 bookmark_key_prefix="yt")
                with tab_shorts:
                    df_s = yt_df_page[yt_df_page.get("isShorts") == True]
                    render_cards(df_s, cols=st.session_state.left_cols,
                                 subtitles=["author","views","durationText","publishedAt","license"],
                                 bookmark_key_prefix="yt_s")
                with tab_video:
                    df_v = yt_df_page[yt_df_page.get("isShorts") == False]
                    render_cards(df_v, cols=st.session_state.left_cols,
                                 subtitles=["author","views","durationText","publishedAt","license"],
                                 bookmark_key_prefix="yt_v")
        with sep:
            st.markdown("<div class='v-sep'>&nbsp;</div>", unsafe_allow_html=True)
        with right:
            st.subheader("ğŸ“¹ Pexels")
            if PEXELS_API_KEY and not px_df_page.empty:
                render_cards(px_df_page, cols=st.session_state.right_cols,
                             subtitles=["author","duration"], bookmark_key_prefix="px")
            else:
                st.info("Pexels API í‚¤ê°€ ì—†ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.link_button("ğŸ”— Pexelsì—ì„œ ê²€ìƒ‰í•˜ê¸°", pexels_search_link(query), use_container_width=True)
                st.caption("â€» ëª©ë¡ì„ ì•±ì—ì„œ ë³´ë ¤ë©´ PEXELS_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    def render_stacked():
        st.subheader("ğŸ¬ YouTube")
        if yt_df_page.empty:
            st.info("YouTube ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            tab_all, tab_shorts, tab_video = st.tabs(["ì „ì²´","ì‡¼ì¸ ","ì˜ìƒ"])
            with tab_all:
                render_cards(yt_df_page, cols=3,
                             subtitles=["author","views","durationText","publishedAt","license"],
                             bookmark_key_prefix="yt")
            with tab_shorts:
                df_s = yt_df_page[yt_df_page.get("isShorts") == True]
                render_cards(df_s, cols=3,
                             subtitles=["author","views","durationText","publishedAt","license"],
                             bookmark_key_prefix="yt_s")
            with tab_video:
                df_v = yt_df_page[yt_df_page.get("isShorts") == False]
                render_cards(df_v, cols=3,
                             subtitles=["author","views","durationText","publishedAt","license"],
                             bookmark_key_prefix="yt_v")
        st.markdown("---")
        st.subheader("ğŸ“¹ Pexels")
        if PEXELS_API_KEY and not px_df_page.empty:
            render_cards(px_df_page, cols=3, subtitles=["author","duration"], bookmark_key_prefix="px")
        else:
            st.info("Pexels API í‚¤ê°€ ì—†ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.link_button("ğŸ”— Pexelsì—ì„œ ê²€ìƒ‰í•˜ê¸°", pexels_search_link(query), use_container_width=True)
            st.caption("â€» ëª©ë¡ì„ ì•±ì—ì„œ ë³´ë ¤ë©´ PEXELS_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    if layout_mode == "ê°€ë¡œ 2íŒ¨ë„":
        render_two_panels()
    else:
        render_stacked()

    # í˜ì´ì§€ë„¤ì´ì…˜ (í•˜ë‹¨)
    page_controls(total_rows, where="bottom")

    st.markdown("---")

    # ë¶ë§ˆí¬ & ë‚´ë³´ë‚´ê¸°
    st.subheader("â­ ë¶ë§ˆí¬")
    if st.session_state.bookmarks:
        bm_df = pd.DataFrame(st.session_state.bookmarks)
        cols_to_show = [c for c in ["platform","title","author","views","url","durationText","publishedAt","license","duration"] if c in bm_df.columns]
        st.dataframe(bm_df[cols_to_show].fillna(""), use_container_width=True, height=260)
        st.download_button(
            "ë¶ë§ˆí¬ CSV ë‹¤ìš´ë¡œë“œ",
            bm_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"minsuk_lab_bookmarks_{int(time.time())}.csv", mime="text/csv"
        )
    else:
        st.caption("ì•„ì§ ë¶ë§ˆí¬ê°€ ì—†ì–´ìš”. ì¹´ë“œì˜ â­ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì¶”ê°€í•˜ì„¸ìš”.")

# Footer legal
st.markdown("<p class='smallcaps'>ë³¸ ë„êµ¬ëŠ” ì›ë³¸ ë§í¬ë¡œë§Œ ì´ë™í•˜ë©° ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>", unsafe_allow_html=True)
