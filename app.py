# app.py
import os
import re
import io
import math
import time
import random
import requests
import pandas as pd
import streamlit as st
from datetime import datetime, timedelta, timezone
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# =========================
# Robust HTTP Session (ì¬ì‹œë„/ì—°ê²° ì¬ì‚¬ìš©)
# =========================
SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "MinsukSearch/3.2", "Accept-Encoding": "gzip"})
_retry = Retry(
    total=3, connect=3, read=3,
    backoff_factor=0.7,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["GET","POST"]
)
_adapter = HTTPAdapter(max_retries=_retry, pool_connections=20, pool_maxsize=60)
SESSION.mount("http://", _adapter)
SESSION.mount("https://", _adapter)

# =========================
# Optional OCR
# =========================
try:
    from PIL import Image
    import pytesseract
    HAS_OCR = True
except Exception:
    HAS_OCR = False

# =========================
# Keys / Const
# =========================
def _split_keys(s: str) -> list[str]:
    return [k.strip() for k in (s or "").split(",") if k.strip()]

# ì—¬ëŸ¬ ê°œ í‚¤ë¥¼ ì½¤ë§ˆë¡œ ë„£ì„ ìˆ˜ ìˆìŒ: YOUTUBE_API_KEY="í‚¤A,í‚¤B,í‚¤C"
YOUTUBE_API_KEYS = _split_keys(st.secrets.get("YOUTUBE_API_KEY", os.getenv("YOUTUBE_API_KEY", "")))
DEEPL_API_KEY    = st.secrets.get("DEEPL_API_KEY", os.getenv("DEEPL_API_KEY", ""))

CSE_API_KEY      = st.secrets.get("CSE_API_KEY", os.getenv("CSE_API_KEY", ""))
CSE_CX           = st.secrets.get("CSE_CX", os.getenv("CSE_CX", ""))

YOUTUBE_SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
YOUTUBE_VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
REQUEST_TIMEOUT    = 12
MAX_YT_PER_QUERY   = 500
PAGE_SIZE_FIXED    = 15  # í•œ í˜ì´ì§€ 15 ê³ ì •

# =========================
# Helpers
# =========================
def http_get(url, params=None, headers=None, timeout=REQUEST_TIMEOUT):
    r = SESSION.get(url, params=params, headers=headers, timeout=timeout)
    r.raise_for_status()
    return r.json()

def http_get_bytes(url, timeout=10):
    r = SESSION.get(url, timeout=timeout)
    r.raise_for_status()
    return r.content

def yt_get(url: str, params: dict, timeout=REQUEST_TIMEOUT):
    """
    YouTube API í˜¸ì¶œ ì‹œ í‚¤ ìë™ ë¡œí…Œì´ì…˜.
    quotaExceeded/403 ë“± ë°œìƒí•˜ë©´ ë‹¤ìŒ í‚¤ë¡œ ë³€ê²½í•˜ì—¬ ì¬ì‹œë„.
    ì„±ê³µí•œ í‚¤ ì¸ë±ìŠ¤ë¥¼ ì„¸ì…˜ì— ê³ ì •.
    """
    if not YOUTUBE_API_KEYS:
        raise RuntimeError("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤. secrets.tomlì— YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    last_err = None
    start_idx = st.session_state.get("yt_key_idx", 0)
    for offset in range(len(YOUTUBE_API_KEYS)):
        idx = (start_idx + offset) % len(YOUTUBE_API_KEYS)
        key = YOUTUBE_API_KEYS[idx]
        p = dict(params); p["key"] = key
        try:
            r = SESSION.get(url, params=p, timeout=timeout)
            if r.status_code == 403:
                try:
                    js = r.json()
                    reason = js.get("error", {}).get("errors", [{}])[0].get("reason", "")
                    if reason == "quotaExceeded":
                        st.warning(f"YouTube í‚¤ #{idx+1} ì¿¼í„° ì†Œì§„. ë‹¤ìŒ í‚¤ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                        continue
                except Exception:
                    pass
            r.raise_for_status()
            st.session_state["yt_key_idx"] = idx
            return r.json()
        except requests.exceptions.HTTPError as e:
            last_err = e
            continue
        except Exception as e:
            last_err = e
            continue
    if last_err:
        raise last_err
    raise RuntimeError("YouTube API ìš”ì²­ ì‹¤íŒ¨(ì›ì¸ ë¶ˆëª…)")

def fmt_int(n):
    try: n = int(n)
    except: return n
    if n >= 1_000_000_000: return f"{n/1_000_000_000:.1f}B"
    if n >= 1_000_000:     return f"{n/1_000_000:.1f}M"
    if n >= 1_000:         return f"{n/1_000:.1f}K"
    return str(n)

def parse_iso8601_duration(s: str) -> int:
    if not s: return 0
    m = re.match(r"^PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?$", s)
    if not m: return 0
    h = int(m.group(1) or 0); mi = int(m.group(2) or 0); se = int(m.group(3) or 0)
    return h*3600 + mi*60 + se

def published_after_from_option(opt: str) -> str | None:
    now = datetime.now(timezone.utc)
    mapping = {
        "ì „ì²´": None, "ìµœê·¼ 24ì‹œê°„": now - timedelta(days=1),
        "ìµœê·¼ 7ì¼": now - timedelta(days=7), "ìµœê·¼ 30ì¼": now - timedelta(days=30),
        "ìµœê·¼ 1ë…„": now - timedelta(days=365),
    }
    dt = mapping.get(opt)
    return dt.isoformat().replace("+00:00","Z") if dt else None

def compute_engagement_score(stats: dict) -> float:
    try:
        v = max(1, int(stats.get("viewCount", 0) or 0))
        l = int(stats.get("likeCount", 0) or 0)
        c = int(stats.get("commentCount", 0) or 0)
        return (l + c) / (v ** 0.85)
    except Exception:
        return 0.0

# ---------- ë²ˆì—­ ìœ í‹¸ ----------
def has_hangul(s: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", s or ""))

def translate_to_ko(text: str) -> str:
    try:
        if not text or has_hangul(text): return text
        if DEEPL_API_KEY:
            url = "https://api-free.deepl.com/v2/translate"
            data = {"auth_key": DEEPL_API_KEY, "text": text, "target_lang": "KO"}
            r = SESSION.post(url, data=data, timeout=10)
            if r.status_code == 200:
                js = r.json()
                trs = js.get("translations", [])
                if trs: return trs[0].get("text", text)
        js = http_get("https://api.mymemory.translated.net/get", {"q": text, "langpair": "en|ko"}, timeout=10)
        if js and js.get("responseData", {}).get("translatedText"):
            return js["responseData"]["translatedText"]
    except Exception:
        pass
    return text

def translate_block_to_ko(text: str) -> str:
    if not text: return text
    parts = re.split(r"(\n{2,})", text)
    out = []
    for p in parts:
        if p.strip()=="" or p.startswith("\n"):
            out.append(p)
        else:
            out.append(translate_to_ko(p))
            time.sleep(0.1)
    return "".join(out)

# =========================
# Age keyword map
# =========================
GE_KEYWORDS = {
    10: ["minecraft","roblox","í¬ì¼“ëª¬","ì• ë‹ˆ","ê²Œì„","ë§ˆì¸í¬ë˜í”„íŠ¸","í‹´","í•™ìƒ","ê³µë¶€ë²•","ê³ ë“±í•™êµ","ì¤‘í•™êµ","ì´ˆë“±í•™êµ","ë¡œë¸”ë¡ìŠ¤"],
    20: ["ëŒ€í•™ìƒ","ë¸Œì´ë¡œê·¸","ì—¬í–‰","ì·¨ì—…","ìì·¨","ì¹´í˜","íŒ¨ì…˜","ì•„ì´ëŒ","kpop","ì—°ì˜ˆ"],
    30: ["ì§ì¥ì¸","ìœ¡ì•„","ì¬í…Œí¬","ë¶€ë™ì‚°","ì¸í…Œë¦¬ì–´","í™ˆì¹´í˜","ì €ì¶•","ìš”ë¦¬","í—¬ìŠ¤"],
    40: ["ê±´ê°•","í‡´ì§","ê°€ì¡±","ê³¨í”„","ë“±ì‚°","ì£¼íƒ","ê°€ì „","ë³´í—˜","í´ë˜ì‹"],
    50: ["ê±´ê°•ê²€ì§„","ê´€ì ˆ","ì€í‡´","ì·¨ë¯¸","ê°€ë“œë‹","ìº í•‘","ìš”ë¦¬","ì—¬í–‰"],
    60: ["ì‹œë‹ˆì–´","ê±´ê°•","ë³µì§€","ì •ì›","í…ƒë°­","ë‚šì‹œ","êµ­ì•…","êµì–‘","ì—­ì‚¬","íŠ¸ë¡œíŠ¸","ì—°ê¸ˆ","ë…¸í›„"],
}
AGE_KEYWORDS = {
    "10ëŒ€": GE_KEYWORDS[10],
    "20ëŒ€": GE_KEYWORDS[20],
    "30ëŒ€": GE_KEYWORDS[30],
    "40ëŒ€": GE_KEYWORDS[40],
    "50ëŒ€": GE_KEYWORDS[50],
    "60ëŒ€": GE_KEYWORDS[60],
}

GENERIC_GAME_NEG = [
    "game","ê²Œì„","ê²œ","ë§ˆì¸í¬ë˜í”„íŠ¸","minecraft","roblox","ë¡œë¸”ë¡ìŠ¤","í¬ì¼“ëª¬",
    "fortnite","genshin","ì›ì‹ ","lol","ë¦¬ê·¸ ì˜¤ë¸Œ ë ˆì „ë“œ","valorant","ë°œë¡œë€íŠ¸",
    "ë°°ê·¸","pubg","steam","ìŠ¤íŒ€","xbox","ps5","í”ŒìŠ¤","ë‹Œí…ë„","nintendo","switch","ìŠ¤ìœ„ì¹˜"
]

def build_age_neg_keywords():
    tags = list(AGE_KEYWORDS.keys())
    neg = {}
    for t in tags:
        others = []
        for t2 in tags:
            if t2 == t: continue
            others.extend(AGE_KEYWORDS[t2])
        neg[t] = sorted(set([o.lower() for o in others]))
    return neg

AGE_NEG_KEYWORDS = build_age_neg_keywords()

def age_relevance_score(title: str, age_tag: str) -> int:
    if not title or age_tag not in AGE_KEYWORDS: return 0
    t = title.lower()
    score = 0
    for kw in AGE_KEYWORDS[age_tag]:
        if kw.lower() in t:
            score += 1
    return score

def age_negative_hit(title: str, age_tag: str) -> bool:
    if not title or age_tag not in AGE_NEG_KEYWORDS: return False
    t = title.lower()
    if any(neg in t for neg in AGE_NEG_KEYWORDS[age_tag]):
        return True
    if age_tag != "10ëŒ€" and any(kw in t for kw in [g.lower() for g in GENERIC_GAME_NEG]):
        return True
    return False

# =========================
# Session
# =========================
def init_state():
    s = st.session_state
    s.setdefault("page", 1)
    s["page_size"] = PAGE_SIZE_FIXED
    s.setdefault("search_history", [])
    s.setdefault("yt_sort", "ì¡°íšŒìˆ˜ìˆœ")
    s.setdefault("accent", "ê¸°ë³¸")
    s.setdefault("yt_fetch_limit", 100)
    s.setdefault("analysis_target", None)
    s.setdefault("trigger_analysis", False)
    s.setdefault("reco_clicks", 0)
    s.setdefault("results_df", pd.DataFrame())
    s.setdefault("last_query", "")
    s.setdefault("last_params", {})
    s.setdefault("age_filter", "ì „ì²´")
    s.setdefault("region_code", "KR")
    s.setdefault("do_search_now", False)
    s.setdefault("yt_key_idx", 0)  # í˜„ì¬ ì‚¬ìš©í•˜ëŠ” í‚¤ ì¸ë±ìŠ¤

    # âœ… ìƒˆë¡œ ì¶”ê°€: ìˆ˜ë™ í˜¸ì¶œ ëª¨ë“œ & ì›í•  ë•Œë§Œ ë¡œë“œ
    s.setdefault("manual_api_mode", True)
    s.setdefault("want_reco_now", False)         # ì¶”ì²œ ë¡œë“œ ë²„íŠ¼ ì‹ í˜¸
    s.setdefault("want_kwboard_now", False)      # í‚¤ì›Œë“œ TOP ë¡œë“œ ë²„íŠ¼ ì‹ í˜¸

# =========================
# Units Estimator (í‘œì‹œìš©)
# =========================
def estimate_units_for_youtube_search(fetch_total: int) -> int:
    """ search.list: 100 units/page(50ê°œ) + videos.list: 1 unit/50ê°œ """
    ft = max(1, min(int(fetch_total), MAX_YT_PER_QUERY))
    pages = math.ceil(ft / 50)         # search.list í˜¸ì¶œ ìˆ˜
    v_chunks = math.ceil(ft / 50)      # videos.list í˜¸ì¶œ ìˆ˜
    return pages * 100 + v_chunks * 1  # ë‹¨ìˆœ ìƒí•œ ì¶”ì •

def estimate_units_for_trending(fetch_total: int = 200) -> int:
    """ videos.list(chart=mostPopular) 1 unit/page """
    pages = math.ceil(fetch_total / 50)
    return pages * 1

def estimate_units_for_kwboard(per_keyword: int = 6, keywords: int = 8) -> int:
    """ keyword_ranked_recos ë‚´ë¶€: ê° í‚¤ì›Œë“œë‹¹ 120ê°œ ìˆ˜ì§‘ (search 3p *100 + videos 3*1) """
    search_pages_per_kw = 3  # 120/50 ë°˜ì˜¬ë¦¼
    return keywords * (search_pages_per_kw * 100 + search_pages_per_kw * 1)

# =========================
# Data: YouTube
# =========================
@st.cache_data(show_spinner=False, ttl=900)
def search_youtube(query: str, *, fetch_total: int, cc_only: bool, upload_window: str,
                   include_channels: list[str], exclude_channels: list[str],
                   include_channel_ids: list[str], exclude_channel_ids: list[str],
                   include_words: list[str], exclude_words: list[str],
                   region_code: str | None, relevance_lang: str | None,
                   safe_mode: str, order_mode: str,
                   duration_param: str,
                   min_seconds: int | None, max_seconds: int | None,
                   age_tag: str = "ì „ì²´"):
    if not YOUTUBE_API_KEYS:
        raise RuntimeError("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤. secrets.tomlì— YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    fetch_total = max(1, min(int(fetch_total), MAX_YT_PER_QUERY))
    per_page = 50
    collected_ids, page_token = [], None

    base_params = {
        "part": "snippet", "q": query, "maxResults": per_page,
        "type": "video", "order": order_mode,
    }
    if cc_only: base_params["videoLicense"] = "creativeCommon"
    if region_code: base_params["regionCode"] = region_code
    if relevance_lang: base_params["relevanceLanguage"] = relevance_lang
    if safe_mode in ("none","moderate","strict"): base_params["safeSearch"] = safe_mode
    if duration_param in ("short","medium","long"): base_params["videoDuration"] = duration_param
    pub_after = published_after_from_option(upload_window)
    if pub_after: base_params["publishedAfter"] = pub_after

    while len(collected_ids) < fetch_total:
        params = dict(base_params)
        if page_token: params["pageToken"] = page_token
        sjson = yt_get(YOUTUBE_SEARCH_URL, params=params)
        items = sjson.get("items", [])
        ids = [it.get("id", {}).get("videoId") for it in items if it.get("id", {}).get("videoId")]
        if not ids: break
        collected_ids.extend(ids)
        page_token = sjson.get("nextPageToken")
        if not page_token or len(collected_ids) >= fetch_total: break

    collected_ids = list(dict.fromkeys(collected_ids))[:fetch_total]
    if not collected_ids: return []

    out = []
    for i in range(0, len(collected_ids), 50):
        chunk = collected_ids[i:i+50]
        params2 = {"part": "snippet,statistics,contentDetails", "id": ",".join(chunk)}
        vjson = yt_get(YOUTUBE_VIDEOS_URL, params=params2)
        for v in vjson.get("items", []):
            vid = v["id"]
            sn  = v.get("snippet", {})
            stt = v.get("statistics", {})
            cd  = v.get("contentDetails", {})
            thumbs = sn.get("thumbnails", {})
            thumb = (thumbs.get("high") or thumbs.get("medium") or thumbs.get("default") or {}).get("url")
            seconds = parse_iso8601_duration(cd.get("duration"))
            is_shorts = seconds <= 60 if seconds else False

            title = sn.get("title") or ""
            channel_title = sn.get("channelTitle", "")
            channel_id = sn.get("channelId", "")
            title_l = title.lower()

            # í•„í„°ë§
            if include_words and not all(w.lower() in title_l for w in include_words): continue
            if exclude_words and any(w.lower() in title_l for w in exclude_words): continue
            if include_channels and channel_title not in include_channels: continue
            if exclude_channels and channel_title in exclude_channels: continue
            if include_channel_ids and channel_id not in include_channel_ids: continue
            if exclude_channel_ids and channel_id in exclude_channel_ids: continue
            if (min_seconds is not None and seconds < min_seconds) or (max_seconds is not None and seconds > max_seconds): continue

            out.append({
                "platform": "YouTube",
                "title": title,
                "author": channel_title,
                "views": int(stt.get("viewCount", 0)) if stt.get("viewCount") else None,
                "url": f"https://www.youtube.com/watch?v={vid}",
                "videoId": vid,
                "thumbnail": thumb,
                "publishedAt": sn.get("publishedAt"),
                "durationSec": seconds,
                "durationText": str(timedelta(seconds=seconds)) if seconds else "",
                "isShorts": is_shorts,
                "description": sn.get("description",""),
            })
        time.sleep(0.12)

    # ê¸°ë³¸ ì •ë ¬
    if st.session_state.yt_sort == "ì¡°íšŒìˆ˜ìˆœ":
        out.sort(key=lambda x: (x["views"] or -1), reverse=True)
    else:
        out.sort(key=lambda x: x.get("publishedAt","") or "", reverse=True)

    # ì—°ë ¹ëŒ€ í•„í„° â€” ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œì™¸ + í•´ë‹¹ ì—°ë ¹ í‚¤ì›Œë“œ ìµœì†Œ 1ê°œ ë§¤ì¹­ ê°•ì œ
    if age_tag != "ì „ì²´":
        out = [r for r in out if not age_negative_hit(r.get("title",""), age_tag)]
        for r in out:
            r["_age_score"] = age_relevance_score(r.get("title",""), age_tag)
        out = [r for r in out if r.get("_age_score", 0) >= 1]
        out.sort(key=lambda x: (x.get("_age_score",0), x.get("views") or 0), reverse=True)

    # dedup
    seen, deduped = set(), []
    for r in out:
        u = r.get("url")
        if u and u not in seen:
            deduped.append(r); seen.add(u)
    return deduped

@st.cache_data(show_spinner=False, ttl=600)
def fetch_trending_with_engagement(region_code: str | None, fetch_total: int, order_mode: str,
                                   age_tag: str = "ì „ì²´", salt: int = 0):
    if not YOUTUBE_API_KEYS: return []
    per_page, collected, page_token = 50, [], None
    region = region_code or "KR"

    while len(collected) < fetch_total:
        params = {"part":"snippet,contentDetails,statistics","chart":"mostPopular","regionCode":region,"maxResults":per_page}
        if page_token: params["pageToken"] = page_token
        try:
            data = yt_get(YOUTUBE_VIDEOS_URL, params=params)
        except requests.exceptions.HTTPError as e:
            reason = ""
            try:
                err = e.response.json()
                reason = err.get("error", {}).get("errors", [{}])[0].get("reason", "")
            except Exception:
                pass
            st.warning(f"íŠ¸ë Œë“œ API í˜¸ì¶œ ì‹¤íŒ¨(í´ë°± ì‚¬ìš©). reason={reason or 'HTTPError'}")
            break

        items = data.get("items", [])
        if not items: break
        for v in items:
            vid = v["id"]; sn=v.get("snippet",{}); stt=v.get("statistics",{}); cd=v.get("contentDetails",{})
            thumbs = sn.get("thumbnails",{})
            thumb = (thumbs.get("high") or thumbs.get("medium") or thumbs.get("default") or {}).get("url")
            seconds = parse_iso8601_duration(cd.get("duration")); is_shorts = seconds<=60 if seconds else False
            collected.append({
                "platform":"YouTube","title":sn.get("title",""),"author":sn.get("channelTitle",""),
                "views": int(stt.get("viewCount",0)) if stt.get("viewCount") else None,
                "url":f"https://www.youtube.com/watch?v={vid}","videoId":vid,"thumbnail":thumb,
                "publishedAt":sn.get("publishedAt"),"durationSec":seconds,"durationText":str(timedelta(seconds=seconds)) if seconds else "",
                "isShorts":is_shorts,"_eng_score":compute_engagement_score(stt),
                "description": sn.get("description",""),
            })
        page_token = data.get("nextPageToken")
        if not page_token or len(collected) >= fetch_total: break

    if order_mode == "viewCount":
        collected.sort(key=lambda x: (x.get("views") or -1), reverse=True)
    else:
        collected.sort(key=lambda x: x.get("publishedAt") or "", reverse=True)

    if age_tag != "ì „ì²´":
        collected = [r for r in collected if not age_negative_hit(r.get("title",""), age_tag)]
        for r in collected:
            r["_age_score"] = age_relevance_score(r.get("title",""), age_tag)
        collected = [r for r in collected if r.get("_age_score", 0) >= 1]
        collected.sort(key=lambda x: (x.get("_age_score",0), x.get("_eng_score",0)), reverse=True)

    if not collected: return []
    pool_size = max(40, int(len(collected) * 0.6))
    pool = collected[:pool_size]
    rnd = random.Random(salt or int(time.time()))
    rnd.shuffle(pool)
    return pool[:max(60, min(120, len(pool)))]

# ---------- í´ë°± & í‚¤ì›Œë“œë³„ ë­í‚¹ ----------
def build_age_seed_queries(age_tag: str, topk: int = 8) -> list[str]:
    keys = AGE_KEYWORDS.get(age_tag, [])
    ban = set(["ê±´ê°•","ì—¬í–‰","ìš”ë¦¬","ê°€ì¡±","ì·¨ë¯¸"])
    qs = [k for k in keys if k not in ban][:topk]
    while len(qs) < topk and len(keys) > len(qs):
        qs.append(keys[len(qs)])
    return qs or ["êµì–‘", "ë‰´ìŠ¤"]

@st.cache_data(show_spinner=False, ttl=600)
def fallback_age_recommendations(age_tag: str, region_code: str, fetch_total_per_q: int = 30) -> list[dict]:
    qs = build_age_seed_queries(age_tag)
    gathered = []
    for q in qs:
        try:
            res = search_youtube(
                q, fetch_total=fetch_total_per_q,
                cc_only=False, upload_window="ìµœê·¼ 1ë…„",
                include_channels=[], exclude_channels=[],
                include_channel_ids=[], exclude_channel_ids=[],
                include_words=[], exclude_words=[],
                region_code=region_code, relevance_lang=None,
                safe_mode="moderate", order_mode="viewCount",
                duration_param="any", min_seconds=None, max_seconds=None,
                age_tag=age_tag,
            )
            gathered.extend(res)
        except Exception:
            continue
    seen, dedup = set(), []
    for r in gathered:
        u = r.get("url")
        if u and u not in seen:
            dedup.append(r); seen.add(u)
    dedup.sort(key=lambda x: (x.get("views") or 0), reverse=True)
    return dedup[:200]

@st.cache_data(show_spinner=False, ttl=600)
def keyword_ranked_recos(age_tag: str, region_code: str, per_keyword: int = 6) -> dict:
    keywords = build_age_seed_queries(age_tag, topk=8)
    board = {}
    for kw in keywords:
        try:
            rows = search_youtube(
                kw, fetch_total=120,
                cc_only=False, upload_window="ìµœê·¼ 1ë…„",
                include_channels=[], exclude_channels=[],
                include_channel_ids=[], exclude_channel_ids=[],
                include_words=[], exclude_words=[],
                region_code=region_code, relevance_lang=None,
                safe_mode="moderate", order_mode="viewCount",
                duration_param="any", min_seconds=None, max_seconds=None,
                age_tag=age_tag,
            )
            board[kw] = rows[:per_keyword]
        except Exception:
            board[kw] = []
    return board

# =========================
# Tokens & OCR for source trace
# =========================
TOKEN_PATTERNS = [
    re.compile(r"@[\w\.\-]{3,}"),
    re.compile(r"#[0-9]{4,}"),
]

def extract_tokens_from_text(txt: str) -> set[str]:
    toks = set()
    if not txt: return toks
    for pat in TOKEN_PATTERNS:
        toks.update(pat.findall(txt))
    return {t for t in toks if len(t) >= 4}

def ocr_tokens_from_thumb(thumb_url: str) -> set[str]:
    if not HAS_OCR or not thumb_url: return set()
    try:
        data = http_get_bytes(thumb_url, timeout=8)
        img = Image.open(io.BytesIO(data))
        text = pytesseract.image_to_string(img, lang="eng")
        return extract_tokens_from_text(text)
    except Exception:
        return set()

def collect_source_tokens(row: dict, *, try_ocr=True) -> set[str]:
    toks = set()
    toks |= extract_tokens_from_text(row.get("title",""))
    toks |= extract_tokens_from_text(row.get("description",""))
    if try_ocr:
        toks |= ocr_tokens_from_thumb(row.get("thumbnail"))
    return {t for t in toks if len(t) >= 4}

# =========================
# External web search (Google CSE ì „ìš©)
# =========================
@st.cache_data(show_spinner=False, ttl=300)
def web_search(query: str, *, num: int = 10):
    results = []
    if CSE_API_KEY and CSE_CX:
        try:
            js = http_get("https://www.googleapis.com/customsearch/v1", {
                "key": CSE_API_KEY, "cx": CSE_CX, "q": query, "num": min(10, num),
                "safe": "off", "hl": "ko"
            })
            for it in js.get("items", []):
                results.append({
                    "title": it.get("title",""),
                    "link": it.get("link",""),
                    "snippet": it.get("snippet","")
                })
        except Exception:
            pass
    return results

def domain_weight(url: str) -> float:
    if not url: return 0
    u = url.lower()
    if "tiktok.com" in u: return 3.0
    if "instagram.com" in u: return 2.5
    if "facebook.com" in u or "fb.watch" in u: return 1.8
    if "x.com" in u or "twitter.com" in u: return 1.6
    if "naver.com" in u or "daum.net" in u: return 1.2
    return 1.0

def extract_keyphrases(text: str, *, topk=5) -> list[str]:
    if not text: return []
    words = re.findall(r"[A-Za-zê°€-í£0-9]{2,}", text.lower())
    stop = set(["the","and","you","for","with","this","that","are","from","ì œ","ê²ƒ","í•´ì„œ","ê·¸ë¦¬ê³ ","í•˜ì§€ë§Œ","ê·¸ëŸ¬ë‚˜","ê·¼ë°","ì´ê±´","ì €ê±´","ì—ì„œ","í•˜ë‹¤"])
    freq = {}
    for w in words:
        if w in stop: continue
        freq[w] = freq.get(w, 0) + 1
    keys = [w for w,_ in sorted(freq.items(), key=lambda x: x[1], reverse=True)]
    keys = [k for k in keys if len(k) >= 3][:topk]
    return keys

def rank_external_results(tokens: set[str], title: str, results: list[dict], extra_keys: list[str]) -> list[dict]:
    if not results: return []
    toks = {t.lower() for t in tokens}
    keys = {k.lower() for k in extra_keys}
    tlo = (title or "").lower()
    ranked = []
    for r in results:
        b = (r.get("title","") + " " + r.get("snippet","")).lower()
        hit_tok = sum(1 for t in toks if t in b)
        hit_key = sum(1 for k in keys if k in b)
        if hit_tok==0 and hit_key==0:
            continue
        dom = domain_weight(r.get("link",""))
        title_sim = 1.0 if any(w for w in [tlo[:20], *tlo.split()[:3]] if w and w in b) else 0.0
        score = hit_tok*2.0 + hit_key*1.2 + dom + title_sim
        r2 = dict(r)
        r2["_ext_score"] = round(score, 3)
        ranked.append(r2)
    ranked.sort(key=lambda x: x["_ext_score"], reverse=True)
    seen_domain = {}
    filtered = []
    for r in ranked:
        d = re.sub(r"^https?://(www\.)?","", r["link"]).split("/")[0]
        c = seen_domain.get(d, 0)
        if c >= 2: continue
        seen_domain[d] = c+1
        filtered.append(r)
    return filtered[:12]

# =========================
# UI helpers â€” DARK ONLY
# =========================
def title_bar():
    left, right1, right2, right3 = st.columns([1,0.13,0.13,0.13])
    with left:
        st.markdown("""
<div class="title-rect">
  <div class="title-rect-inner">ğŸ› ï¸ ë¯¼ì„ì´ì˜ ì‘ì—…ì‹¤</div>
</div>
""", unsafe_allow_html=True)
        st.caption("ì›ë³¸ ë§í¬ë¡œë§Œ ì´ë™í•˜ëŠ” ì•ˆì „í•œ ê°œì¸ìš© ê²€ìƒ‰ ë„êµ¬ (ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì—†ìŒ)")
    btn_html = """
    <div class="title-action-wrap">
      <a href="{href}" target="_blank" class="btn-link">
        {label}
      </a>
    </div>
    """
    with right1:
        st.markdown(btn_html.format(href="https://ssyoutube.online/ko/youtube-video-downloader-ko/", label="ìœ íŠœë¸Œë‹¤ìš´"), unsafe_allow_html=True)
    with right2:
        st.markdown(btn_html.format(href="https://snaptik.kim/", label="í‹±í†¡ë‹¤ìš´"), unsafe_allow_html=True)
    with right3:
        st.markdown(btn_html.format(href="https://www.pexels.com/videos/", label="Pexels"), unsafe_allow_html=True)

def thumb_with_badge(url, duration_text, views):
    dur = duration_text or ""
    vtx = fmt_int(views) if views is not None else ""
    html = f"""
<div class="thumb-wrap">
  <img src="{url}" class="thumb-img"/>
  <div class="badge-wrap">
    <span class="badge">{'â± ' + dur if dur else ''}</span>
    <span class="badge">{'ğŸ‘€ ' + vtx if vtx else ''}</span>
  </div>
</div>
"""
    st.markdown(html, unsafe_allow_html=True)

def insta_card_wrap(func):
    def inner(*args, **kwargs):
        st.markdown("<div class='card-grid'>", unsafe_allow_html=True)
        func(*args, **kwargs)
        st.markdown("</div>", unsafe_allow_html=True)
    return inner

def page_controls(total_count: int, where: str):
    total_pages = max(1, math.ceil(total_count / st.session_state.page_size))
    l, c, r = st.columns([1,2,1])
    with c:
        nav = st.columns(4, gap="small")
        with nav[0]:
            st.button("â® ì²˜ìŒ", key=f"{where}_first", on_click=lambda: st.session_state.update(page=1))
        with nav[1]:
            st.button("â—€ ì´ì „", key=f"{where}_prev", on_click=lambda: st.session_state.update(page=max(1, st.session_state.page-1)))
        with nav[2]:
            st.button("ë‹¤ìŒ â–¶", key=f"{where}_next", on_click=lambda: st.session_state.update(page=min(total_pages, st.session_state.page+1)))
        with nav[3]:
            st.button("ë§ˆì§€ë§‰ â­", key=f"{where}_last", on_click=lambda: st.session_state.update(page=total_pages))
        st.caption(f"í˜ì´ì§€ {st.session_state.page} / {total_pages} Â· í•œ í˜ì´ì§€ {st.session_state.page_size}ê°œ")

def slice_df_for_page(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    ps = st.session_state.page_size; pg = st.session_state.page
    return df.iloc[(pg-1)*ps : (pg-1)*ps + ps]

# =========================
# CSS â€” ë‹¤í¬ëª¨ë“œë§Œ ìœ ì§€
# =========================
def inject_css(accent="#00A389"):
    st.markdown(f"""
<style>
:root {{
  --accent: {accent};
  --grad-start: #ff7ac6; --grad-mid: #ffb86b; --grad-end: #8be9fd;
  --btn-radius: 999px;
}}
.stApp {{
  background: linear-gradient(180deg, #0b0d12 0%, #0f1117 100%);
  color: #eaeef5 !important;
}}
.block-container {{ padding-top: 2.2cm; }}
a {{ color: var(--accent) !important; text-decoration: none; }}
.card-grid {{ margin-top: 4px; }}
div, p, span, label, h1, h2, h3, h4, h5, h6 {{ color:#eaeef5 !important; }}

/* Title */
.title-rect {{ display:inline-block; padding:4px;
  background: linear-gradient(135deg, var(--grad-start), var(--grad-mid), var(--grad-end)); border-radius:10px; }}
.title-rect-inner {{
  padding:8px 14px; background: rgba(12,14,20,0.9); border-radius:8px;
  font-size:1.8rem; font-weight:800; color:#eaeef5;
}}
.title-action-wrap {{ margin-top:18px; text-align:right; }}

/* Buttons */
.btn-link {{
  display:inline-block; padding:8px 12px;
  background: linear-gradient(135deg, var(--grad-start), var(--grad-mid), var(--grad-end));
  color:#fff !important; border-radius:var(--btn-radius); font-weight:700;
  box-shadow:0 4px 14px rgba(0,0,0,0.28); text-align:center; font-size:.82rem;
}}
.btn-link.small {{ padding:8px 12px; font-size:.82rem; }}
.stButton>button {{
  border-radius:var(--btn-radius);
  background: linear-gradient(135deg, var(--grad-start), var(--grad-mid), var(--grad-end));
  border:none; color:white; font-weight:700; font-size:.86rem; padding:9px 14px;
  box-shadow:0 6px 16px rgba(0,0,0,0.28);
}}
.card-grid .stButton>button {{ font-size:.48rem; padding:6px 10px; line-height:1.05; }}
.analysis-back .stButton>button {{ font-size:1.05rem; padding:14px 18px; width:100%;
  box-shadow:0 8px 18px rgba(0,0,0,0.28); }}

/* Thumb badge */
.thumb-wrap {{ position:relative; }}
.thumb-img {{ width:100%; border-radius:16px; display:block; }}
.badge-wrap {{ position:absolute; top:8px; right:8px; display:flex; gap:6px; }}
.badge {{ padding:4px 8px; border-radius:999px; font-weight:800; font-size:.72rem;
  background: rgba(0,0,0,.55); color:#fff; }}

/* Expander */
details {{ border-radius:12px; background:#0f1320; border:1px solid #1f2537; padding:6px 10px; }}
details[open] {{ background:#0c101b; }}

/* Pagination center */
div[data-testid="column"] > div:has(> .stButton) {{ display:flex; justify-content:center; }}

</style>
""", unsafe_allow_html=True)

# =========================
# Renderers
# =========================
@insta_card_wrap
def render_cards(df: pd.DataFrame, *, cols: int, subtitles: list[str], bookmark_key_prefix: str):
    if df is None or df.empty:
        st.info("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."); return
    rows = math.ceil(len(df)/cols)
    for r in range(rows):
        ccols = st.columns(cols, gap="large")
        for i in range(cols):
            idx = r*cols + i
            if idx >= len(df): break
            row = df.iloc[idx].to_dict()
            with ccols[i]:
                if row.get("thumbnail"):
                    thumb_with_badge(row["thumbnail"], row.get("durationText",""), row.get("views"))
                title = row.get("title", "Untitled"); url = row.get("url", "#")
                st.markdown(f"<div style='font-weight:800; font-size:1.02rem; margin:6px 0 2px'>{title}</div>", unsafe_allow_html=True)

                chips=[]
                for key in subtitles:
                    val=row.get(key)
                    if val in (None,"",0): continue
                    if key=="author": chips.append(f"ì œì‘ì {val}")
                    elif key=="views": chips.append(f"ì¡°íšŒìˆ˜ {fmt_int(val)}")
                    elif key=="durationText": chips.append(f"ê¸¸ì´ {val}")
                    elif key=="publishedAt": chips.append(f"ê²Œì‹œ {str(val)[:10]}")
                if chips: st.caption(" Â· ".join(chips))

                # â¬‡ï¸ ë²„íŠ¼ 2ê°œë§Œ: ì›ë³¸ë§í¬ / ì˜ìƒë¶„ì„ (ì›ë³¸ì°¾ê¸° ë²„íŠ¼ ì œê±°)
                b1, b2 = st.columns(2, gap="small")
                with b1:
                    st.markdown(f"""<a href="{url}" target="_blank" class="btn-link small" style="display:block;text-align:center;">ì›ë³¸ë§í¬</a>""", unsafe_allow_html=True)
                with b2:
                    def _open_analysis(r=row):
                        st.query_params["view"] = "analysis"
                        st.query_params["vid"]  = r.get("videoId","")
                        st.session_state["analysis_target"] = r
                        st.session_state["trigger_analysis"] = True
                        st.rerun()
                    st.button("ì˜ìƒë¶„ì„", key=f"an_{bookmark_key_prefix}_{idx}_{abs(hash(url))%10_000_000}", use_container_width=True, on_click=_open_analysis)

def render_results(df_all: pd.DataFrame):
    total_rows = len(df_all)
    if total_rows == 0:
        st.info("YouTube ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."); return
    page_controls(total_rows, where="top")
    df_page = slice_df_for_page(df_all)

    st.subheader("ğŸ¬ YouTube")
    tab_all, tab_shorts, tab_video = st.tabs(["ì „ì²´","ì‡¼ì¸ ","ì˜ìƒ"])
    with tab_all:
        render_cards(df_page, cols=3, subtitles=["author","views","durationText","publishedAt"], bookmark_key_prefix="yt")
    with tab_shorts:
        df_s = df_page.loc[df_page["isShorts"]==True] if "isShorts" in df_page.columns else df_page.iloc[0:0]
        render_cards(df_s, cols=3, subtitles=["author","views","durationText","publishedAt"], bookmark_key_prefix="yt_s")
    with tab_video:
        df_v = df_page.loc[df_page["isShorts"]==False] if "isShorts" in df_page.columns else df_page.iloc[0:0]
        render_cards(df_v, cols=3, subtitles=["author","views","durationText","publishedAt"], bookmark_key_prefix="yt_v")

    page_controls(total_rows, where="bottom")
    st.markdown("---")

# =========================
# Analysis View
# =========================
def fetch_transcript_any(video_id: str):
    try:
        from youtube_transcript_api import (
            YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound, CouldNotRetrieveTranscript
        )
    except Exception:
        return (None, None)
    try:
        try:
            tr = YouTubeTranscriptApi.get_transcript(video_id, languages=["ko"])
            txt = "\n".join([s.get("text","").strip() for s in tr if s.get("text")])
            if txt.strip(): return (txt, "ko")
        except Exception: pass
        tr = YouTubeTranscriptApi.get_transcript(video_id, languages=["en"])
        txt = "\n".join([s.get("text","").strip() for s in tr if s.get("text")])
        if txt.strip(): return (txt, "en")
    except (TranscriptsDisabled, NoTranscriptFound, CouldNotRetrieveTranscript):
        return (None, None)
    except Exception:
        return (None, None)

def chunk_transcript(transcript: str, max_chars=300) -> list[dict]:
    chunks, buf = [], []
    for line in transcript.splitlines():
        if len(" ".join(buf+[line])) > max_chars:
            chunks.append({"idx": len(chunks)+1, "text": " ".join(buf)}); buf=[line]
        else:
            buf.append(line)
    if buf: chunks.append({"idx": len(chunks)+1, "text":" ".join(buf)})
    return chunks

def heuristic_prompts(title: str, transcript: str | None):
    title_s = (title or "").strip()
    base_kw = []
    if transcript:
        words = re.findall(r"[A-Za-zê°€-í£0-9]+", transcript.lower())
        stop = set(["ê·¸ë¦¬ê³ ","ê·¸ë˜ì„œ","í•˜ì§€ë§Œ","ê·¸ëŸ¬ë‚˜","ê·¸ëƒ¥","ê·¼ë°","ì´ê±´","ì €ê±´","ì—ì„œ","í•˜ë‹¤","the","and","to","of","in","a","is"])
        freq = {}
        for w in words:
            if len(w)<=1 or w in stop: continue
            freq[w] = freq.get(w,0)+1
        base_kw = [w for w,_ in sorted(freq.items(), key=lambda x:x[1], reverse=True)[:5]]
    hook = f"{title_s[:40]}? í•µì‹¬ë§Œ ì§‘ì–´ì„œ ë§í• ê²Œìš”." if title_s else "í•µì‹¬ë§Œ ì§‘ì–´ì„œ ë§í• ê²Œìš”."
    problem = "ì‚¬ëŒë“¤ì´ ë†“ì¹˜ëŠ” í¬ì¸íŠ¸ë¥¼ ì§§ê²Œ ì •ë¦¬í•´ë³¼ê¹Œìš”?"
    solution = f"í‚¤ì›Œë“œ: {', '.join(base_kw)}" if base_kw else "í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ë¦¬ê³  ë©”ì‹œì§€ë¥¼ ì••ì¶•í•˜ì„¸ìš”."
    cta = "ë„ì›€ëë‹¤ë©´ ì €ì¥í•˜ê³  ë‹¤ìŒ ì•„ì´ë””ì–´ë¡œ ì´ì–´ê°€ìš”."
    shorts_script = f"í›„í‚¹: {hook}\në¬¸ì œ: {problem}\ní•´ê²°: {solution}\nCTA: {cta}"
    image_prompt = f'í¬í† ë¦¬ì–¼, ë°ì€ í†¤, ì£¼ì œ: "{title_s}", í•µì‹¬ì–´: {", ".join(base_kw) if base_kw else "ê°„ê²°/ì„ ëª…/ì§‘ì¤‘"}'
    return shorts_script, image_prompt

def render_analysis_view(row: dict):
    st.markdown('<div class="analysis-back">', unsafe_allow_html=True)
    if st.button("âœ– ë‹«ê¸°(ëª©ë¡ìœ¼ë¡œ)", use_container_width=True, key=f"back_{row.get('videoId','')}"):
        st.query_params.clear()
        st.session_state["trigger_analysis"] = False
        st.rerun()
    st.subheader("ğŸ§  ì˜ìƒ ë¶„ì„")

    c1, c2 = st.columns([1,1])
    with c1:
        thumb_with_badge(row.get("thumbnail"), row.get("durationText",""), row.get("views"))
    with c2:
        raw_title = row.get("title",""); ko_title = translate_to_ko(raw_title)
        if ko_title and ko_title != raw_title:
            st.markdown(f"**ì œëª©(ë²ˆì—­):** {ko_title}"); st.caption(f"ì›ë¬¸ ì œëª©: {raw_title}")
        else:
            st.markdown(f"**ì œëª©:** {raw_title}")
        st.caption(f"ì±„ë„: {row.get('author','')} Â· ì¡°íšŒìˆ˜: {fmt_int(row.get('views')) if row.get('views') is not None else 'â€”'}")
        st.caption(f"ê²Œì‹œ: {str(row.get('publishedAt') or '')[:10]} Â· ê¸¸ì´: {row.get('durationText','')}")
        st.markdown(f"""<a href="{row.get("url")}" target="_blank" class="btn-link small">ì›ë³¸ ì˜ìƒ ì—´ê¸°</a>""", unsafe_allow_html=True)

        # ğŸ‘‰ ë¶„ì„ ë‚´ë¶€ â€˜ì›ë³¸ì°¾ê¸°â€™ ë²„íŠ¼ (ìˆì¸ ì¼ ë•Œë§Œ í™œì„±)
        is_shorts = bool(row.get("isShorts"))
        col_a1, col_a2 = st.columns(2)
        with col_a1:
            st.caption(" ")
        with col_a2:
            def _open_trace_from_analysis(r=row):
                if not r.get("isShorts"):
                    st.warning("â€˜ì›ë³¸ì°¾ê¸°â€™ëŠ” ìˆì¸ ì—ì„œë§Œ ì œê³µë©ë‹ˆë‹¤.")
                    return
                st.session_state["analysis_target"] = r
                st.query_params["view"] = "trace"
                st.query_params["vid"]  = r.get("videoId","")
                st.rerun()

            st.button(
                "ğŸ§­ ì›ë³¸ì°¾ê¸° (ìˆì¸  ì „ìš©)",
                key=f"btn_trace_in_analysis_{row.get('videoId','')}",
                disabled=not is_shorts,
                use_container_width=True,
                on_click=_open_trace_from_analysis
            )

    vid = row.get("videoId")
    if not vid:
        st.info("ì˜ìƒ IDë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    with st.status("ìë§‰ í™•ì¸/ë²ˆì—­ ì¤‘â€¦", expanded=True) as status:
        st.markdown('<span class="loading-badge"><span class="loading-dot"></span> ì²˜ë¦¬ ì¤‘â€¦</span>', unsafe_allow_html=True)
        transcript, lang = fetch_transcript_any(vid)
        status.update(label="ì™„ë£Œ", state="complete")

    if not transcript:
        st.warning("ìë§‰ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); return

    show_text = transcript if lang == "ko" else translate_block_to_ko(transcript)
    st.markdown("### ğŸ“ ìë§‰ / íƒ€ì„ë¼ì¸(ìš”ì•½)")
    chunks = chunk_transcript(show_text, max_chars=350)
    for ch in chunks[:12]:
        with st.expander(f"ì„¹ì…˜ {ch['idx']}", expanded=False):
            st.write(ch["text"])

    st.markdown("### âœï¸ ìˆì¸  ëŒ€ë³¸ ì”¨ì•— & ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´)")
    shorts_script, image_prompt = heuristic_prompts(row.get("title",""), show_text)
    st.text_area("ìˆì¸  ëŒ€ë³¸(í›„í‚¹/ë¬¸ì œ/í•´ê²°/CTA)", shorts_script, height=150, key=f"seed_script_{vid}")
    st.text_area("ì´ë¯¸ì§€/ì˜ìƒ í”„ë¡¬í”„íŠ¸ ì‹œë“œ", image_prompt, height=100, key=f"seed_prompt_{vid}")

# =========================
# Trace View (ìˆì¸  ì›ë³¸ì°¾ê¸°)
# =========================
def render_trace_view(row: dict):
    if st.button("âœ– ë‹«ê¸°(ëª©ë¡ìœ¼ë¡œ)", use_container_width=True, key=f"trace_close_{row.get('videoId','')}"):
        st.query_params.clear()
        st.rerun()

    st.subheader("ğŸ§­ ì›ë³¸ ì†ŒìŠ¤ ì¶”ì  (ìˆì¸  â†’ ì™¸ë¶€ ì‚¬ì´íŠ¸)")

    # âœ… ìˆ˜ë™ ëª¨ë“œì¼ ë•Œ: ë²„íŠ¼ ëˆŒëŸ¬ì•¼ ì™¸ë¶€ ê²€ìƒ‰(CSE) ì‹œì‘
    if st.session_state.get("manual_api_mode", True):
        if not st.button("ğŸŒ ì›¹ì—ì„œ ì›ë³¸ í›„ë³´ ê²€ìƒ‰ ì‹œì‘", use_container_width=True, key=f"btn_trace_go_{row.get('videoId','')}"):
            st.info("ì›ë³¸ í›„ë³´ ê²€ìƒ‰ì„ ì‹œì‘í•˜ë ¤ë©´ ìœ„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            return

    with st.status("ì¶œì²˜ í† í° ì¶”ì¶œ ì¤‘â€¦", expanded=True) as status:
        tokens = collect_source_tokens(row, try_ocr=True)
        status.update(label="í† í° ì¶”ì¶œ ì™„ë£Œ", state="complete")

    title_keys = extract_keyphrases(row.get("title",""), topk=4)
    desc_keys  = extract_keyphrases(row.get("description",""), topk=3)
    key_str = " ".join(title_keys[:2] + desc_keys[:2])

    if not tokens and not key_str:
        st.warning("í† í°ê³¼ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ë¥¸ ì˜ìƒìœ¼ë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        return

    st.caption("ê°ì§€ëœ í† í°: " + (" Â· ".join(sorted(tokens)) if tokens else "â€”"))
    st.caption("í•µì‹¬ í‚¤ì›Œë“œ: " + (key_str if key_str else "â€”"))

    token_q = " OR ".join(sorted(tokens))[:180] if tokens else ""
    site_pool = ["tiktok.com","instagram.com","facebook.com","x.com","twitter.com","reddit.com","9gag.com","imgur.com","bilibili.com","tv.naver.com","kakao.tv"]
    all_results = []

    with st.status("ì›¹ì—ì„œ ì›ë³¸ í›„ë³´ ê²€ìƒ‰ ì¤‘â€¦", expanded=True) as status:
        if not (CSE_API_KEY and CSE_CX):
            st.error("ì™¸ë¶€ ì›¹ê²€ìƒ‰ í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. secrets.tomlì— CSE_API_KEY, CSE_CXë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
            return

        for site in site_pool:
            if token_q and key_str:
                q = f"({token_q}) {key_str} site:{site}"
            elif token_q:
                q = f"({token_q}) site:{site}"
            else:
                q = f"{key_str} site:{site}"
            res = web_search(q, num=10)
            all_results.extend(res)
            time.sleep(0.1)

        if token_q and key_str:
            all_results.extend(web_search(f"({token_q}) {key_str}", num=10))
        elif token_q:
            all_results.extend(web_search(f"({token_q})", num=10))
        else:
            all_results.extend(web_search(key_str, num=10))

        status.update(label="í›„ë³´ ìˆ˜ì§‘ ì™„ë£Œ", state="complete")

    ranked = rank_external_results(tokens, row.get("title",""), all_results, extra_keys=(title_keys+desc_keys))

    if not ranked:
        st.info("í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”.")
        return

    st.markdown("#### í›„ë³´ ë¦¬ìŠ¤íŠ¸")
    for i, cand in enumerate(ranked[:12], 1):
        st.markdown(f"**#{i}. ì ìˆ˜ {cand['_ext_score']}** â€” {cand.get('title','')}")
        st.caption(cand.get("snippet",""))
        st.markdown(f"""<a href="{cand.get("link")}" target="_blank" class="btn-link small">ì›ë³¸(í›„ë³´) ì—´ê¸°</a>""", unsafe_allow_html=True)
        st.markdown("---")

# =========================
# APP
# =========================
st.set_page_config(page_title="ë¯¼ì„ì´ì˜ ì‘ì—…ì‹¤", page_icon="ğŸ› ï¸", layout="wide")
init_state()

# Accent
accent_map = {"ê¸°ë³¸":"#00A389","ë¸”ë£¨":"#2F80ED","ê·¸ë¦°":"#27AE60","í•‘í¬":"#EB5757","ë³´ë¼":"#A259FF"}
accent = accent_map.get(st.session_state.accent, "#00A389")

# CSS + Title
inject_css(accent)
title_bar()

# ===== ì‚¬ì´ë“œë°” =====
with st.sidebar:
    st.header("ê²€ìƒ‰ ì˜µì…˜")

    # âœ… ìˆ˜ë™ í˜¸ì¶œ ëª¨ë“œ í† ê¸€
    s = st.session_state
    s.manual_api_mode = st.toggle(
        "ìˆ˜ë™ ëª¨ë“œ(ì›í•  ë•Œë§Œ API í˜¸ì¶œ)",
        value=s.get("manual_api_mode", True),
        help="ì¼œë‘ë©´ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œë§Œ YouTube APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."
    )

    # ìµœê·¼ ê²€ìƒ‰
    if st.session_state.search_history:
        st.caption("ìµœê·¼ ê²€ìƒ‰")
        hist_cols = st.columns(3)
        for i, qv in enumerate(st.session_state.search_history[-9:][::-1]):
            with hist_cols[i % 3]:
                if st.button(qv, key=f"hist_{i}"):
                    st.session_state["_prefill_query"] = qv
                    st.session_state["sb_query"] = qv
                    # ìˆ˜ë™ ëª¨ë“œì—ì„œëŠ” ìë™ íŠ¸ë¦¬ê±° ì•ˆí•¨
                    if not s.manual_api_mode:
                        st.session_state["do_search_now"] = True

    q_default = st.session_state.get("_prefill_query", "prank")

    # ğŸ”§ ê²€ìƒ‰ í¼
    with st.form(key="search_form", clear_on_submit=False):
        query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value=q_default, key="sb_query")
        submit_search = st.form_submit_button("ê²€ìƒ‰", use_container_width=True, type="primary")

    st.markdown("---")
    st.subheader("YouTube ì„¤ì •")
    yt_cc = st.checkbox("í¬ë¦¬ì—ì´í‹°ë¸Œ ì»¤ë¨¼ì¦ˆ(CC)ë§Œ", value=False, key="sb_cc")

    st.caption("Â· ê²°ê³¼ ìˆ˜ì§‘ëŸ‰ (ë§ì´ ì˜¬ë¦´ìˆ˜ë¡ ëŠë ¤ì§ˆ ìˆ˜ ìˆì–´ìš”)")
    st.session_state.yt_fetch_limit = st.selectbox(
        "YouTube ê²°ê³¼ ìˆ˜ì§‘ëŸ‰(ìµœëŒ€)",
        options=[50,100,200,300,400,500],
        index=[50,100,200,300,400,500].index(st.session_state.yt_fetch_limit)
              if st.session_state.yt_fetch_limit in [50,100,200,300,400,500] else 1,
        key="sb_fetch_limit"
    )
    st.caption(f"Â· í•œ í˜ì´ì§€ ë³´ê¸°: **{PAGE_SIZE_FIXED}ê°œ ê³ ì •**")

    yt_upload_window = st.selectbox(
        "ì—…ë¡œë“œ ì‹œì ",
        ["ì „ì²´","ìµœê·¼ 24ì‹œê°„","ìµœê·¼ 7ì¼","ìµœê·¼ 30ì¼","ìµœê·¼ 1ë…„"],
        index=0, key="sb_uploadwin"
    )

    region_label = st.selectbox(
        "ì§€ì—­",
        ["ìë™","í•œêµ­(KR)","ë¯¸êµ­(US)","ì¼ë³¸(JP)","ì˜êµ­(GB)","ë…ì¼(DE)","í”„ë‘ìŠ¤(FR)","ì¸ë„(IN)","ì¸ë„ë„¤ì‹œì•„(ID)","ë¸Œë¼ì§ˆ(BR)","ë©•ì‹œì½”(MX)"],
        index=1, key="sb_region"
    )
    region_map = {"ìë™":"", "í•œêµ­(KR)":"KR", "ë¯¸êµ­(US)":"US", "ì¼ë³¸(JP)":"JP", "ì˜êµ­(GB)":"GB",
                  "ë…ì¼(DE)":"DE", "í”„ë‘ìŠ¤(FR)":"FR", "ì¸ë„(IN)":"IN", "ì¸ë„ë„¤ì‹œì•„(ID)":"ID", "ë¸Œë¼ì§ˆ(BR)":"BR", "ë©•ì‹œì½”(MX)":"MX"}
    region_code = region_map[region_label] or "KR"
    st.session_state["region_code"] = region_code

    lang_label = st.selectbox(
        "ì–¸ì–´",
        ["ìë™","í•œêµ­ì–´(ko)","ì˜ì–´(en)","ì¼ë³¸ì–´(ja)","ìŠ¤í˜ì¸ì–´(es)","í”„ë‘ìŠ¤ì–´(fr)","ë…ì¼ì–´(de)","ì¸ë„ë„¤ì‹œì•„ì–´(id)","í¬ë¥´íˆ¬ê°ˆì–´(pt)","íŒë””ì–´(hi)"],
        index=1, key="sb_lang"
    )
    lang_map = {"ìë™":"", "í•œêµ­ì–´(ko)":"ko", "ì˜ì–´(en)":"en", "ì¼ë³¸ì–´(ja)":"ja", "ìŠ¤í˜ì¸ì–´(es)":"es", "í”„ë‘ìŠ¤ì–´(fr)":"fr", "ë…ì¼ì–´(de)":"de",
                "ì¸ë„ë„¤ì‹œì•„ì–´(id)":"id", "í¬ë¥´íˆ¬ê°ˆì–´(pt)":"pt", "íŒë””ì–´(hi)":"hi"}
    relevance_lang = lang_map[lang_label]

    ylen_label = st.selectbox("ê¸¸ì´ í•„í„°", ["ì „ì²´","ì§§ìŒ(<4ë¶„)","ì¤‘ê°„(4~20ë¶„)","ê¸´(>20ë¶„)"], index=0, key="sb_ylen")
    ylen_map = {"ì „ì²´":"any","ì§§ìŒ(<4ë¶„)":"short","ì¤‘ê°„(4~20ë¶„)":"medium","ê¸´(>20ë¶„)":"long"}
    duration_param = ylen_map[ylen_label]

    c1, c2 = st.columns(2)
    with c1:
        min_sec = st.number_input("ìµœì†Œ ê¸¸ì´(ì´ˆ)", min_value=0, max_value=86400, value=0, step=5, key="sb_minsec")
        min_seconds = None if min_sec==0 else int(min_sec)
    with c2:
        max_sec = st.number_input("ìµœëŒ€ ê¸¸ì´(ì´ˆ)", min_value=0, max_value=86400, value=0, step=5, key="sb_maxsec")
        max_seconds = None if max_sec==0 else int(max_sec)

    inc = st.text_input("í¬í•¨ ì±„ë„ëª…(ì‰¼í‘œ)", value="", key="sb_inc_chname")
    exc = st.text_input("ì œì™¸ ì±„ë„ëª…(ì‰¼í‘œ)", value="", key="sb_exc_chname")
    inc_ids = st.text_input("í¬í•¨ ì±„ë„ID(ì‰¼í‘œ)", value="", key="sb_inc_chid")
    exc_ids = st.text_input("ì œì™¸ ì±„ë„ID(ì‰¼í‘œ)", value="", key="sb_exc_chid")
    include_channels = [s.strip() for s in inc.split(",") if s.strip()]
    exclude_channels = [s.strip() for s in exc.split(",") if s.strip()]
    include_channel_ids = [s.strip() for s in inc_ids.split(",") if s.strip()]
    exclude_channel_ids = [s.strip() for s in exc_ids.split(",") if s.strip()]

    inc_words = st.text_input("ì œëª©ì— ë°˜ë“œì‹œ í¬í•¨(ì‰¼í‘œ)", value="", key="sb_inc_words")
    exc_words = st.text_input("ì œëª©ì— í¬í•¨ë˜ë©´ ì œì™¸(ì‰¼í‘œ)", value="", key="sb_exc_words")
    include_words = [s.strip() for s in inc_words.split(",") if s.strip()]
    exclude_words = [s.strip() for s in exc_words.split(",") if s.strip()]

    # =========================
    # ğŸ”¢ ìœ ë‹› ì˜ˆìƒ ìœ„ì ¯
    # =========================
    st.markdown("---")
    st.subheader("ìœ ë‹› ì‚¬ìš©ëŸ‰(ëŒ€ëµ) ë¯¸ë¦¬ë³´ê¸°")
    est_search = estimate_units_for_youtube_search(st.session_state.yt_fetch_limit)
    est_trend  = estimate_units_for_trending(fetch_total=200)
    est_kw     = estimate_units_for_kwboard(per_keyword=6, keywords=8)
    st.caption("â€» search.listëŠ” 50ê°œ/í˜ì´ì§€ë‹¹ 100 units, videos.listëŠ” 50ê°œ/í˜ì´ì§€ë‹¹ 1 unit ê¸°ì¤€ ì¶”ì •")
    st.metric("í˜„ì¬ ê²€ìƒ‰ ì‹¤í–‰ ì‹œ", f"~ {est_search:,} units")
    st.metric("ì¶”ì²œ ë¡œë“œ(íŠ¸ë Œë“œ 200ê°œ)", f"~ {est_trend:,} units")
    st.metric("í‚¤ì›Œë“œë³„ TOP ë¡œë“œ", f"~ {est_kw:,} units")
    if st.session_state.yt_fetch_limit >= 300:
        st.warning("ê²€ìƒ‰ ìˆ˜ì§‘ëŸ‰ì´ í½ë‹ˆë‹¤. ìœ ë‹› ì†Œëª¨ê°€ ë§ì´ ë°œìƒí•  ìˆ˜ ìˆì–´ìš”.")

# ===== ê³µí†µ: ê²€ìƒ‰ ì‹¤í–‰ í•¨ìˆ˜
def perform_search(q: str):
    order_mode = "viewCount" if st.session_state.yt_sort=="ì¡°íšŒìˆ˜ìˆœ" else "date"
    try:
        yt_all = search_youtube(
            q,
            fetch_total=st.session_state.yt_fetch_limit,
            cc_only=st.session_state.get("sb_cc", False),
            upload_window=st.session_state.get("sb_uploadwin","ì „ì²´"),
            include_channels=include_channels, exclude_channels=exclude_channels,
            include_channel_ids=include_channel_ids, exclude_channel_ids=exclude_channel_ids,
            include_words=include_words, exclude_words=exclude_words,
            region_code=st.session_state.get("region_code","KR"),
            relevance_lang=(relevance_lang or None),
            safe_mode="moderate", order_mode=order_mode,
            duration_param=duration_param, min_seconds=min_seconds, max_seconds=max_seconds,
            age_tag=st.session_state.get("age_filter","ì „ì²´"),
        )
        df_all = pd.DataFrame(yt_all)
        st.session_state.results_df = df_all
        st.session_state.last_query = q
        st.session_state.last_params = {
            "region_code": st.session_state.get("region_code","KR"), "relevance_lang": relevance_lang, "duration_param": duration_param,
            "min_seconds": min_seconds, "max_seconds": max_seconds,
            "include_channels": include_channels, "exclude_channels": exclude_channels,
            "include_channel_ids": include_channel_ids, "exclude_channel_ids": exclude_channel_ids,
            "include_words": include_words, "exclude_words": exclude_words,
        }
        st.session_state.page = 1
        return True
    except Exception as e:
        msg = str(e)
        if "quotaExceeded" in msg or "403 Client Error" in msg:
            st.error("YouTube ê²€ìƒ‰ ì¿¼í„°ê°€ ëª¨ë‘ ì†Œì§„ë˜ì—ˆì–´ìš”. YOUTUBE_API_KEYì— ì—¬ëŸ¬ í‚¤ë¥¼ ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œí…Œì´ì…˜í•©ë‹ˆë‹¤. (ì˜ˆ: í‚¤A,í‚¤B,í‚¤C)")
        else:
            st.error(f"YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return False

# ===== ìƒë‹¨ íˆ´ë°” =====
with st.container():
    tb1 = st.columns([1.0])[0]
    with tb1:
        st.session_state.yt_sort = st.radio("YouTube ì •ë ¬", ["ì¡°íšŒìˆ˜ìˆœ","ìµœì‹ ìˆœ"], horizontal=True,
                                            index=["ì¡°íšŒìˆ˜ìˆœ","ìµœì‹ ìˆœ"].index(st.session_state.yt_sort), key="tb_sort")

    # ì—°ë ¹ëŒ€ ë¹ ë¥¸ í•„í„° â€” í´ë¦­ ì‹œ í•„í„°ë§Œ ë°”ê¾¸ê³ , ë¡œë“œëŠ” ë²„íŠ¼ìœ¼ë¡œ
    st.caption("ì—°ë ¹ëŒ€ ì„ íƒ í›„, ì•„ë˜ â€˜ì¶”ì²œ ë¡œë“œ / í‚¤ì›Œë“œ TOP ë¡œë“œâ€™ ë²„íŠ¼ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    chip_row = st.columns(6)
    for i, a in enumerate(["10ëŒ€","20ëŒ€","30ëŒ€","40ëŒ€","50ëŒ€","60ëŒ€"]):
        with chip_row[i]:
            if st.button(a, key=f"age_{a}"):
                st.session_state.age_filter = a
                st.session_state.results_df = pd.DataFrame()
                st.session_state["reco_clicks"] = 0
                st.query_params.clear()
                # ìë™ ëª¨ë“œë¼ë©´ ë°”ë¡œ ë¡œë“œ í”Œë˜ê·¸ ì„¸íŒ…
                if not st.session_state.get("manual_api_mode", True):
                    st.session_state["want_reco_now"] = True
                st.rerun()

# ===== ì¶”ì²œ/í‚¤ì›Œë“œ TOP ìˆ˜ë™ ë¡œë“œ ë²„íŠ¼
if st.session_state.get("manual_api_mode", True):
    col_reco, col_kw = st.columns(2)
    with col_reco:
        if st.button("ğŸ² ì¶”ì²œ ë¡œë“œ", use_container_width=True, key="btn_load_reco"):
            st.session_state["want_reco_now"] = True
            st.rerun()
    with col_kw:
        if st.button("ğŸ” í‚¤ì›Œë“œ TOP ë¡œë“œ", use_container_width=True, key="btn_load_kwboard"):
            st.session_state["want_kwboard_now"] = True
            st.rerun()

# =========================
# ëœë¤ì¶”ì²œ + í‚¤ì›Œë“œë³„ TOP (ë²„íŠ¼ìœ¼ë¡œ ì§„ì…)
# =========================
if st.session_state.get("want_reco_now", False):
    st.session_state["want_reco_now"] = False
    st.markdown("## ğŸ² ëœë¤ì¶”ì²œ (íŠ¸ë Œë“œÃ—ì°¸ì—¬ë„)")
    with st.status("ì¶”ì²œ ëª©ë¡ ë¡œë”© ì¤‘â€¦", expanded=True) as status:
        st.markdown('<span class="loading-badge"><span class="loading-dot"></span> ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘</span>', unsafe_allow_html=True)
        order_mode = "viewCount" if st.session_state.yt_sort=="ì¡°íšŒìˆ˜ìˆœ" else "date"

        reco_candidates = fetch_trending_with_engagement(
            region_code=st.session_state.get("region_code","KR"),
            fetch_total=200,
            order_mode=order_mode,
            age_tag=st.session_state.get("age_filter","ì „ì²´"),
            salt=st.session_state.get("reco_clicks",0)
        )

        if not reco_candidates or len(reco_candidates) < 12:
            fb = fallback_age_recommendations(
                age_tag=st.session_state.get("age_filter","ì „ì²´"),
                region_code=st.session_state.get("region_code","KR"),
                fetch_total_per_q=30
            )
            reco_candidates = fb

        status.update(label="ì™„ë£Œ", state="complete")

    if not reco_candidates:
        st.info("ì¶”ì²œ í›„ë³´ë¥¼ ë§Œë“¤ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë„“í˜€ì„œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
    else:
        def _rank_key(x):
            return (x.get("_age_score", 0), x.get("_eng_score", 0), x.get("views") or 0)
        top_sorted = sorted(reco_candidates, key=_rank_key, reverse=True)
        top_n = top_sorted[:12]

        st.subheader("ğŸ† ì—°ë ¹ëŒ€ TOP 12")
        top_df = pd.DataFrame(top_n)
        render_cards(top_df, cols=3, subtitles=["author","views","durationText","publishedAt"], bookmark_key_prefix="reco_top")

        remain = [r for r in reco_candidates if r not in top_n]
        if remain:
            rnd = random.Random(time.time_ns() ^ st.session_state.get("reco_clicks",0))
            display = rnd.sample(remain, k=min(18, len(remain)))
            st.subheader("ğŸ”€ ë¬´ì‘ìœ„ ì¶”ì²œ")
            st.caption("â€» ì—°ë ¹ í•„í„° + (ìˆë‹¤ë©´) ì°¸ì—¬ë„/ì¡°íšŒìˆ˜ ìƒìœ„ê¶Œì˜ í° í’€ì—ì„œ ë¬´ì‘ìœ„ ì¶”ì¶œ")
            reco_df = pd.DataFrame(display)
            render_cards(reco_df, cols=3, subtitles=["author","views","durationText","publishedAt"], bookmark_key_prefix="reco")
    st.markdown("---")

# âœ… í‚¤ì›Œë“œë³„ ë­í‚¹ ë³´ë“œ (ë²„íŠ¼ ëˆŒë €ì„ ë•Œë§Œ)
if st.session_state.get("want_kwboard_now", False):
    st.session_state["want_kwboard_now"] = False
    st.markdown("## ğŸ” í‚¤ì›Œë“œë³„ TOP")
    with st.status("í‚¤ì›Œë“œë³„ ìƒìœ„ ì˜ìƒ ìˆ˜ì§‘ ì¤‘â€¦", expanded=False) as status2:
        board = keyword_ranked_recos(
            age_tag=st.session_state.get("age_filter","ì „ì²´"),
            region_code=st.session_state.get("region_code","KR"),
            per_keyword=6
        )
        status2.update(label="ì™„ë£Œ", state="complete")
    for kw, rows in board.items():
        if not rows:
            continue
        st.markdown(f"### #{kw} ìƒìœ„")
        df_kw = pd.DataFrame(rows)
        render_cards(df_kw, cols=3, subtitles=["author","views","durationText","publishedAt"], bookmark_key_prefix=f"kw_{kw}")
    st.markdown("---")

# =========================
# ê²€ìƒ‰ ì‹¤í–‰: (1) í¼ ì œì¶œ, (2) ìë™ëª¨ë“œ íˆìŠ¤í† ë¦¬
# =========================
if submit_search or st.session_state.get("do_search_now", False):
    st.session_state["do_search_now"] = False
    current_q = st.session_state.get("sb_query", "").strip()
    if current_q:
        st.session_state["reco_clicks"] = 0  # ìˆ˜ë™ ê²€ìƒ‰ ì‹œ ì¶”ì²œ ì„¹ì…˜ ìˆ¨ê¹€
        if current_q not in st.session_state.search_history:
            st.session_state.search_history.append(current_q)
        if len(st.session_state.search_history) > 10:
            st.session_state.search_history = st.session_state.search_history[-10:]
        with st.status("YouTube ê²€ìƒ‰ ì¤‘â€¦", expanded=True) as status:
            st.markdown('<span class="loading-badge"><span class="loading-dot"></span> API í˜¸ì¶œ ì¤‘â€¦</span>', unsafe_allow_html=True)
            ok = perform_search(current_q)
            if ok: status.update(label="ì™„ë£Œ", state="complete")
            else:  status.update(label="ì˜¤ë¥˜", state="error")

# ê²°ê³¼ ë Œë”(í˜ì´ì§€ ì´ë™ ì‹œ ì¬ê²€ìƒ‰ ì—†ì´ ê³„ì† ë³´ì´ê²Œ)
if not st.session_state.results_df.empty and st.query_params.get("view","") not in ("analysis","trace"):
    render_results(st.session_state.results_df)

# =========================
# ë¼ìš°íŒ…: analysis / trace
# =========================
qp_view = st.query_params.get("view","")
if qp_view == "analysis":
    target = st.session_state.get("analysis_target")
    if not target:
        vid = st.query_params.get("vid","")
        if vid and not st.session_state.results_df.empty:
            row = st.session_state.results_df.loc[st.session_state.results_df["videoId"]==vid]
            if not row.empty: target = row.iloc[0].to_dict()
    if target:
        render_analysis_view(target)
    else:
        st.warning("ë¶„ì„ ëŒ€ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª©ë¡ì—ì„œ ë‹¤ì‹œ ì—´ì–´ì£¼ì„¸ìš”.")

elif qp_view == "trace":
    target = st.session_state.get("analysis_target")
    if not target:
        vid = st.query_params.get("vid","")
        if vid and not st.session_state.results_df.empty:
            row = st.session_state.results_df.loc[st.session_state.results_df["videoId"]==vid]
            if not row.empty: target = row.iloc[0].to_dict()
    if target and target.get("isShorts"):
        render_trace_view(target)
    elif target and not target.get("isShorts"):
        st.info("â€˜ì›ë³¸ì°¾ê¸°â€™ëŠ” ìˆì¸ ì—ì„œë§Œ ì œê³µë©ë‹ˆë‹¤. ì‡¼ì¸ ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
    else:
        st.warning("ëŒ€ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª©ë¡ì—ì„œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

# Footer
st.markdown("<p style='font-variant-caps: all-small-caps; letter-spacing: .03em; opacity: .85;'>ë³¸ ë„êµ¬ëŠ” ì›ë³¸ ë§í¬ë¡œë§Œ ì´ë™í•˜ë©° ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>", unsafe_allow_html=True)
