import os
import re
import math
import time
import requests
import pandas as pd
import streamlit as st
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus

# =========================
# Secrets / Keys
# =========================
YOUTUBE_API_KEY  = st.secrets.get("YOUTUBE_API_KEY", os.getenv("YOUTUBE_API_KEY", ""))
PEXELS_API_KEY   = st.secrets.get("PEXELS_API_KEY",  os.getenv("PEXELS_API_KEY",  ""))
REDDIT_UA        = st.secrets.get("REDDIT_USER_AGENT", os.getenv("REDDIT_USER_AGENT", "ShortsFinder/1.0 by minsuk"))

# =========================
# Endpoints / Const
# =========================
YOUTUBE_SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
YOUTUBE_VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
PEXELS_SEARCH_URL  = "https://api.pexels.com/videos/search"
PEXELS_POPULAR_URL = "https://api.pexels.com/videos/popular"
REQUEST_TIMEOUT    = 12
MAX_YT_PER_QUERY   = 500   # YouTube Search API ê´€ë¡€ìƒ ìµœëŒ€ 500ê°œ ì •ë„ ë°˜í™˜

# =========================
# Helpers
# =========================
def http_get(url, params=None, headers=None, timeout=REQUEST_TIMEOUT, max_retries=2, backoff=0.8):
    last_err = None
    for i in range(max_retries + 1):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=timeout)
            if r.status_code == 200:
                try:
                    return r.json()
                except Exception:
                    last_err = "Invalid JSON response"
            else:
                last_err = f"HTTP {r.status_code}: {r.text[:200]}"
        except Exception as e:
            last_err = str(e)
        time.sleep(backoff * (2 ** i))
    raise RuntimeError(last_err or "Unknown network error")

def fmt_int(n):
    try:
        n = int(n)
    except:
        return n
    if n >= 1_000_000_000: return f"{n/1_000_000_000:.1f}B"
    if n >= 1_000_000:     return f"{n/1_000_000:.1f}M"
    if n >= 1_000:         return f"{n/1_000:.1f}K"
    return str(n)

def parse_iso8601_duration(s: str) -> int:
    if not s: return 0
    m = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", s)
    if not m: return 0
    h = int(m.group(1) or 0)
    mi = int(m.group(2) or 0)
    se = int(m.group(3) or 0)
    return h*3600 + mi*60 + se

def published_after_from_option(opt: str) -> str | None:
    now = datetime.now(timezone.utc)
    mapping = {
        "ì „ì²´": None,
        "ìµœê·¼ 24ì‹œê°„": now - timedelta(days=1),
        "ìµœê·¼ 7ì¼":    now - timedelta(days=7),
        "ìµœê·¼ 30ì¼":   now - timedelta(days=30),
        "ìµœê·¼ 1ë…„":    now - timedelta(days=365),
    }
    dt = mapping.get(opt)
    return dt.isoformat().replace("+00:00", "Z") if dt else None

def pexels_search_link(query: str) -> str:
    return f"https://www.pexels.com/search/videos/{quote_plus(query)}/"

# =========================
# Session
# =========================
def init_state():
    s = st.session_state
    s.setdefault("page", 1)
    s.setdefault("page_size", 10)
    s.setdefault("bookmarks", [])
    s.setdefault("search_history", [])
    s.setdefault("yt_sort", "ì¡°íšŒìˆ˜ìˆœ")   # ë˜ëŠ” "ìµœì‹ ìˆœ"
    s.setdefault("left_cols", 2)         # YT ì¹´ë“œ ì—´ ìˆ˜
    s.setdefault("right_cols", 2)        # Pexels ì¹´ë“œ ì—´ ìˆ˜
    s.setdefault("accent", "ê¸°ë³¸")
    s.setdefault("yt_fetch_limit", 100)  # YouTube ê²°ê³¼ ìˆ˜ì§‘ëŸ‰ ê¸°ë³¸

def page_controls(total_count: int, where: str):
    total_pages = max(1, math.ceil(total_count / st.session_state.page_size))
    cols = st.columns([1,1,2,2,1,1])
    with cols[0]:
        st.button("â®ï¸ ì²˜ìŒ", key=f"{where}_first", on_click=lambda: st.session_state.update(page=1))
    with cols[1]:
        st.button("â—€ï¸ ì´ì „", key=f"{where}_prev", on_click=lambda: st.session_state.update(page=max(1, st.session_state.page-1)))
    with cols[2]:
        # í‘œì‹œ ì „ìš© (ì‹¤ì œ ë³€ê²½ì€ ì‚¬ì´ë“œë°”)
        _ = st.selectbox(
            "í•œ í˜ì´ì§€ ë³´ê¸°",
            options=[10,20,50],
            index=[10,20,50].index(st.session_state.page_size) if st.session_state.page_size in (10,20,50) else 0,
            key=f"{where}_pagesize_display"
        )
    with cols[3]:
        st.session_state.page = st.number_input(
            f"í˜ì´ì§€ (ì´ {total_pages}p)",
            min_value=1, max_value=total_pages, value=min(st.session_state.page, total_pages),
            step=1, key=f"{where}_pnum"
        )
    with cols[4]:
        st.button("ë‹¤ìŒ â–¶ï¸", key=f"{where}_next", on_click=lambda: st.session_state.update(page=min(total_pages, st.session_state.page+1)))
    with cols[5]:
        st.button("ë§ˆì§€ë§‰ â­ï¸", key=f"{where}_last", on_click=lambda: st.session_state.update(page=total_pages))

def slice_df_for_page(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    ps = st.session_state.page_size
    pg = st.session_state.page
    start = (pg - 1) * ps
    end = start + ps
    return df.iloc[start:end]

def render_cards(df: pd.DataFrame, *, cols: int, subtitles: list[str], bookmark_key_prefix: str):
    if df is None or df.empty:
        st.info("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    rows = math.ceil(len(df) / cols)
    for r in range(rows):
        ccols = st.columns(cols)
        for i in range(cols):
            idx = r*cols + i
            if idx >= len(df): break
            row = df.iloc[idx].to_dict()
            with ccols[i]:
                if row.get("thumbnail"):
                    st.image(row["thumbnail"], use_container_width=True)
                title = row.get("title", "Untitled")
                url = row.get("url", "#")
                st.markdown(f"**[{title}]({url})**", unsafe_allow_html=True)

                chips = []
                for key in subtitles:
                    val = row.get(key)
                    if val in (None, "", 0): continue
                    if key == "author":          chips.append(f"ì œì‘ì {val}")
                    elif key == "views":         chips.append(f"ì¡°íšŒìˆ˜ {fmt_int(val)}")
                    elif key == "duration":      chips.append(f"ê¸¸ì´ {val}s")
                    elif key == "durationText":  chips.append(f"ê¸¸ì´ {val}")
                    elif key == "publishedAt":   chips.append(f"ê²Œì‹œ {str(val)[:10]}")
                    elif key == "license":       chips.append(f"ë¼ì´ì„ ìŠ¤ {val}")
                if chips:
                    st.caption(" Â· ".join(chips))

                # ë¶ë§ˆí¬
                book_id = f"{bookmark_key_prefix}_{idx}_{hash(url)%10_000_000}"
                def _add_bm(r=row):
                    st.session_state.bookmarks.append(r)
                    # ì¦‰ì‹œ dedup
                    try:
                        bm_df = pd.DataFrame(st.session_state.bookmarks)
                        if "url" in bm_df.columns:
                            bm_df = bm_df.drop_duplicates(subset=["url"], keep="first")
                            st.session_state.bookmarks = bm_df.to_dict(orient="records")
                    except Exception:
                        pass
                st.button("â­ ë¶ë§ˆí¬", key=book_id, on_click=_add_bm)

def _filter_shorts(df: pd.DataFrame, flag: bool) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if "isShorts" not in df.columns:
        return df.iloc[0:0]
    return df.loc[df["isShorts"] == flag]

# =========================
# Data Sources
# =========================
@st.cache_data(show_spinner=False, ttl=900)
def search_youtube(query: str, *, fetch_total: int, cc_only: bool, upload_window: str,
                   include_channels: list[str], exclude_channels: list[str],
                   include_channel_ids: list[str], exclude_channel_ids: list[str],
                   include_words: list[str], exclude_words: list[str],
                   region_code: str | None, relevance_lang: str | None,
                   safe_mode: str, order_mode: str,
                   duration_param: str,
                   min_seconds: int | None, max_seconds: int | None):
    """
    YouTube Searchë¥¼ nextPageTokenìœ¼ë¡œ ë°˜ë³µ í˜¸ì¶œí•˜ì—¬ ìµœëŒ€ fetch_totalê°œê¹Œì§€ ìˆ˜ì§‘.
    ì´í›„ videos APIë¡œ ìƒì„¸ ì •ë³´(í†µê³„/ê¸¸ì´) ë°°ì¹˜ ì¡°íšŒ(50ê°œ ë‹¨ìœ„).
    """
    if not YOUTUBE_API_KEY:
        raise RuntimeError("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤. secrets.tomlì— YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    fetch_total = max(1, min(int(fetch_total), MAX_YT_PER_QUERY))  # 1~500
    per_page = 50  # Search API ìµœëŒ€
    collected_ids = []
    page_token = None

    base_params = {
        "part": "snippet",
        "q": query,
        "maxResults": per_page,
        "type": "video",
        "key": YOUTUBE_API_KEY,
        "order": order_mode,  # "viewCount" or "date"
    }
    if cc_only:
        base_params["videoLicense"] = "creativeCommon"
    if region_code:
        base_params["regionCode"] = region_code
    if relevance_lang:
        base_params["relevanceLanguage"] = relevance_lang
    if safe_mode in ("none","moderate","strict"):
        base_params["safeSearch"] = safe_mode
    if duration_param in ("any","short","medium","long") and duration_param != "any":
        base_params["videoDuration"] = duration_param

    pub_after = published_after_from_option(upload_window)
    if pub_after:
        base_params["publishedAfter"] = pub_after

    # ------- Search ë£¨í”„ (ìµœëŒ€ fetch_total ë˜ëŠ” nextPageToken ì†Œì§„) -------
    while len(collected_ids) < fetch_total:
        params = dict(base_params)
        if page_token:
            params["pageToken"] = page_token

        sjson = http_get(YOUTUBE_SEARCH_URL, params=params)
        items = sjson.get("items", [])
        ids = [it.get("id", {}).get("videoId") for it in items if it.get("id", {}).get("videoId")]
        if not ids:
            break
        collected_ids.extend(ids)

        page_token = sjson.get("nextPageToken")
        if not page_token:
            break

        # ì•ˆì „: ë„ˆë¬´ í° ë£¨í”„ ë°©ì§€
        if len(collected_ids) >= fetch_total:
            break

    # í•„ìš”ëŸ‰ìœ¼ë¡œ ìë¥´ê¸°
    collected_ids = list(dict.fromkeys(collected_ids))[:fetch_total]
    if not collected_ids:
        return []

    # ------- Videos ìƒì„¸ ì¡°íšŒ (50ê°œ ë‹¨ìœ„) -------
    out = []
    for i in range(0, len(collected_ids), 50):
        chunk = collected_ids[i:i+50]
        params2 = {
            "part": "snippet,statistics,contentDetails",
            "id": ",".join(chunk),
            "key": YOUTUBE_API_KEY
        }
        vjson = http_get(YOUTUBE_VIDEOS_URL, params=params2)
        for v in vjson.get("items", []):
            vid = v["id"]
            sn  = v.get("snippet", {})
            stt = v.get("statistics", {})
            cd  = v.get("contentDetails", {})
            thumbs = sn.get("thumbnails", {})
            thumb = (thumbs.get("medium") or thumbs.get("high") or thumbs.get("default") or {}).get("url")

            channel_title = sn.get("channelTitle", "")
            channel_id    = sn.get("channelId", "")
            seconds = parse_iso8601_duration(cd.get("duration"))
            is_shorts = seconds <= 60 if seconds else False

            title_l = (sn.get("title") or "").lower()

            # í¬í•¨/ì œì™¸ í•„í„°ë§
            if include_words and not all(w.lower() in title_l for w in include_words):
                continue
            if exclude_words and any(w.lower() in title_l for w in exclude_words):
                continue
            if include_channels and channel_title not in include_channels:
                continue
            if exclude_channels and channel_title in exclude_channels:
                continue
            if include_channel_ids and channel_id not in include_channel_ids:
                continue
            if exclude_channel_ids and channel_id in exclude_channel_ids:
                continue
            if (min_seconds is not None and seconds < min_seconds) or (max_seconds is not None and seconds > max_seconds):
                continue

            out.append({
                "platform": "YouTube",
                "title": sn.get("title"),
                "author": channel_title,
                "views": int(stt.get("viewCount", 0)) if stt.get("viewCount") else None,
                "url": f"https://www.youtube.com/watch?v={vid}",
                "thumbnail": thumb,
                "publishedAt": sn.get("publishedAt"),
                "durationSec": seconds,
                "durationText": str(timedelta(seconds=seconds)) if seconds else "",
                "isShorts": is_shorts,
                "license": "CC" if cc_only else "Standard/Unknown",
            })

    # í‘œì‹œ ì •ë ¬(ë³´ì¡°)
    if st.session_state.yt_sort == "ì¡°íšŒìˆ˜ìˆœ":
        out.sort(key=lambda x: (x["views"] or -1), reverse=True)
    else:
        out.sort(key=lambda x: x.get("publishedAt", "") or "", reverse=True)

    # URL ê¸°ì¤€ dedup (í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ì œê±°)
    seen = set()
    deduped = []
    for r in out:
        u = r.get("url")
        if u and u not in seen:
            deduped.append(r)
            seen.add(u)
    return deduped

@st.cache_data(show_spinner=False, ttl=900)
def search_pexels(query: str, *, fetch: int, mode="search"):
    if not PEXELS_API_KEY:
        return []
    headers = {"Authorization": PEXELS_API_KEY}
    url = PEXELS_POPULAR_URL if mode == "popular" else PEXELS_SEARCH_URL
    params = {"per_page": min(fetch, 80)}
    if mode == "search":
        params["query"] = query
    data = http_get(url, params=params, headers=headers)
    out = []
    for v in data.get("videos", []):
        out.append({
            "platform": "Pexels",
            "title": f"Pexels Video by {v.get('user',{}).get('name','Creator')}",
            "author": v.get("user",{}).get("name"),
            "views": None,
            "url": v.get("url"),
            "thumbnail": v.get("image"),
            "duration": v.get("duration"),
        })
    return out

# =========================
# UI
# =========================
st.set_page_config(page_title="ë¯¼ì„ì´ì˜ ì‘ì—…ì‹¤", page_icon="ğŸ› ï¸", layout="wide")
init_state()

# Accent Theme (CSS)
accent_map = {
    "ê¸°ë³¸": "#00A389",
    "ë¸”ë£¨": "#2F80ED",
    "ê·¸ë¦°": "#27AE60",
    "í•‘í¬": "#EB5757",
    "ë³´ë¼": "#A259FF",
}
accent = accent_map.get(st.session_state.accent, "#00A389")

# ---- CSS (ì œëª© ì˜ë¦¼ ë°©ì§€ + í…Œë§ˆ ìƒ‰) ----
st.markdown(f"""
<style>
:root {{ --accent: {accent}; }}
.block-container {{ padding-top: 24px; }}
.v-sep {{ border-left: 1px solid #ddd; height: 100%; opacity: 0.4; }}
h1, .stApp h1 {{ margin: 8px 0 6px 0 !important; line-height: 1.28; }}
a {{ color: var(--accent) !important; }}
.stButton>button, .stDownloadButton button {{
  border-radius: 999px; border: 1px solid var(--accent);
  color: var(--accent); background: transparent;
}}
.stButton>button:hover, .stDownloadButton button:hover {{
  background: var(--accent); color: white;
}}
</style>
""", unsafe_allow_html=True)

# ì œëª© (ìŠ¤í˜ì´ì„œ + íƒ€ì´í‹€)
st.markdown("<div style='height:16px'></div>", unsafe_allow_html=True)
st.title("ğŸ› ï¸ ë¯¼ì„ì´ì˜ ì‘ì—…ì‹¤")
st.caption("ì›ë³¸ ë§í¬ë¡œë§Œ ì´ë™í•˜ëŠ” ì•ˆì „í•œ ê°œì¸ìš© ê²€ìƒ‰ ë„êµ¬ (ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì—†ìŒ)")

# ===== ì‚¬ì´ë“œë°” (í•œê¸€ UI, ëª¨ë“  ìœ„ì ¯ì— key ë¶€ì—¬) =====
with st.sidebar:
    st.header("ê²€ìƒ‰ ì˜µì…˜")

    # ìµœê·¼ ê²€ìƒ‰
    if st.session_state.search_history:
        st.caption("ìµœê·¼ ê²€ìƒ‰")
        hist_cols = st.columns(3)
        for i, qv in enumerate(st.session_state.search_history[-9:][::-1]):
            with hist_cols[i % 3]:
                if st.button(qv, key=f"hist_{i}"):
                    st.session_state["_prefill_query"] = qv

    # ê²€ìƒ‰ì–´ + (ë§¨ ìœ„ ì‹¤í–‰ ë²„íŠ¼)
    q_default = st.session_state.get("_prefill_query", "prank")
    query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value=q_default, key="sb_query")
    st.session_state["_prefill_query"] = query

    run = st.button("ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True, type="primary", key="run_top")

    st.markdown("---")

    # í™”ë©´ ë°°ì¹˜
    layout_mode = st.selectbox("í™”ë©´ ë°°ì¹˜", ["ê°€ë¡œ 2íŒ¨ë„", "ìƒí•˜ ìŠ¤íƒ"], index=0, key="sb_layout")

    # ë¯¼ê°ë„(YouTube safeSearch)
    safe_label = st.selectbox("ë¯¼ê°ë„ í•„í„°(YouTube)", ["ê°•í•¨", "ë³´í†µ", "í•´ì œ"], index=1, key="sb_safe")
    yt_safe = {"ê°•í•¨": "strict", "ë³´í†µ": "moderate", "í•´ì œ": "none"}[safe_label]

    # YouTube ì„¤ì •
    st.subheader("YouTube ì„¤ì •")
    yt_cc   = st.checkbox("í¬ë¦¬ì—ì´í‹°ë¸Œ ì»¤ë¨¼ì¦ˆ(CC)ë§Œ", value=False, key="sb_cc")
    yt_upload_window = st.selectbox("ì—…ë¡œë“œ ì‹œì ",
                                    ["ì „ì²´", "ìµœê·¼ 24ì‹œê°„", "ìµœê·¼ 7ì¼", "ìµœê·¼ 30ì¼", "ìµœê·¼ 1ë…„"],
                                    index=0, key="sb_uploadwin")

    # ì§€ì—­/ì–¸ì–´ (í•œêµ­ì–´ ë¼ë²¨ â†’ ì½”ë“œ ë§¤í•‘)
    region_label = st.selectbox("ì§€ì—­",
                                ["ìë™", "í•œêµ­(KR)", "ë¯¸êµ­(US)", "ì¼ë³¸(JP)", "ì˜êµ­(GB)", "ë…ì¼(DE)", "í”„ë‘ìŠ¤(FR)", "ì¸ë„(IN)", "ì¸ë„ë„¤ì‹œì•„(ID)", "ë¸Œë¼ì§ˆ(BR)", "ë©•ì‹œì½”(MX)"],
                                index=1, key="sb_region")
    region_map = {"ìë™":"", "í•œêµ­(KR)":"KR", "ë¯¸êµ­(US)":"US", "ì¼ë³¸(JP)":"JP", "ì˜êµ­(GB)":"GB", "ë…ì¼(DE)":"DE", "í”„ë‘ìŠ¤(FR)":"FR",
                  "ì¸ë„(IN)":"IN", "ì¸ë„ë„¤ì‹œì•„(ID)":"ID", "ë¸Œë¼ì§ˆ(BR)":"BR", "ë©•ì‹œì½”(MX)":"MX"}
    region_code = region_map[region_label]

    lang_label = st.selectbox("ì–¸ì–´",
                              ["ìë™", "í•œêµ­ì–´(ko)", "ì˜ì–´(en)", "ì¼ë³¸ì–´(ja)", "ìŠ¤í˜ì¸ì–´(es)", "í”„ë‘ìŠ¤ì–´(fr)", "ë…ì¼ì–´(de)", "ì¸ë„ë„¤ì‹œì•„ì–´(id)", "í¬ë¥´íˆ¬ê°ˆì–´(pt)", "íŒë””ì–´(hi)"],
                              index=1, key="sb_lang")
    lang_map = {"ìë™":"", "í•œêµ­ì–´(ko)":"ko", "ì˜ì–´(en)":"en", "ì¼ë³¸ì–´(ja)":"ja", "ìŠ¤í˜ì¸ì–´(es)":"es", "í”„ë‘ìŠ¤ì–´(fr)":"fr", "ë…ì¼ì–´(de)":"de",
                "ì¸ë„ë„¤ì‹œì•„ì–´(id)":"id", "í¬ë¥´íˆ¬ê°ˆì–´(pt)":"pt", "íŒë””ì–´(hi)":"hi"}
    relevance_lang = lang_map[lang_label]

    # ê¸¸ì´ í•„í„° (ë¼ë²¨â†’API ê°’)
    ylen_label = st.selectbox("ê¸¸ì´ í•„í„°", ["ì „ì²´", "ì§§ìŒ(<4ë¶„)", "ì¤‘ê°„(4~20ë¶„)", "ê¸´(>20ë¶„)"], index=0, key="sb_ylen")
    ylen_map = {"ì „ì²´":"any", "ì§§ìŒ(<4ë¶„)":"short", "ì¤‘ê°„(4~20ë¶„)":"medium", "ê¸´(>20ë¶„)":"long"}
    duration_param = ylen_map[ylen_label]

    # ì´ˆ ë‹¨ìœ„ ê¸¸ì´ ë²”ìœ„
    c1, c2 = st.columns(2)
    with c1:
        min_sec = st.number_input("ìµœì†Œ ê¸¸ì´(ì´ˆ)", min_value=0, max_value=86400, value=0, step=5, key="sb_minsec")
        min_seconds = None if min_sec == 0 else int(min_sec)
    with c2:
        max_sec = st.number_input("ìµœëŒ€ ê¸¸ì´(ì´ˆ)", min_value=0, max_value=86400, value=0, step=5, key="sb_maxsec")
        max_seconds = None if max_sec == 0 else int(max_sec)

    # ì±„ë„/í‚¤ì›Œë“œ í•„í„°
    inc = st.text_input("í¬í•¨ ì±„ë„ëª…(ì‰¼í‘œ)", value="", key="sb_inc_chname")
    exc = st.text_input("ì œì™¸ ì±„ë„ëª…(ì‰¼í‘œ)", value="", key="sb_exc_chname")
    inc_ids = st.text_input("í¬í•¨ ì±„ë„ID(ì‰¼í‘œ)", value="", key="sb_inc_chid")
    exc_ids = st.text_input("ì œì™¸ ì±„ë„ID(ì‰¼í‘œ)", value="", key="sb_exc_chid")
    include_channels = [s.strip() for s in inc.split(",") if s.strip()]
    exclude_channels = [s.strip() for s in exc.split(",") if s.strip()]
    include_channel_ids = [s.strip() for s in inc_ids.split(",") if s.strip()]
    exclude_channel_ids = [s.strip() for s in exc_ids.split(",") if s.strip()]

    inc_words = st.text_input("ì œëª©ì— ë°˜ë“œì‹œ í¬í•¨(ì‰¼í‘œ)", value="", key="sb_inc_words")
    exc_words = st.text_input("ì œëª©ì— í¬í•¨ë˜ë©´ ì œì™¸(ì‰¼í‘œ)", value="", key="sb_exc_words")
    include_words = [s.strip() for s in inc_words.split(",") if s.strip()]
    exclude_words = [s.strip() for s in exc_words.split(",") if s.strip()]

    # YouTube ê²°ê³¼ ìˆ˜ì§‘ëŸ‰
    st.subheader("YouTube ê²°ê³¼ ìˆ˜ì§‘ëŸ‰")
    st.session_state.yt_fetch_limit = st.selectbox(
        "ìµœëŒ€ ê°€ì ¸ì˜¬ ê°œìˆ˜",
        options=[50,100,200,300,400,500],
        index=[50,100,200,300,400,500].index(st.session_state.yt_fetch_limit) if st.session_state.yt_fetch_limit in [50,100,200,300,400,500] else 1,
        key="sb_fetch_limit"
    )

    st.markdown("---")

    # í˜ì´ì§€ í¬ê¸° (ì‚¬ì´ë“œë°”ì—ì„œë§Œ ìƒíƒœ ë³€ê²½)
    st.subheader("í•œ í˜ì´ì§€ ë³´ê¸°")
    st.session_state.page_size = st.selectbox("í‘œì‹œ ê°œìˆ˜", [10,20,50],
                                              index=[10,20,50].index(st.session_state.page_size),
                                              key="sb_pagesize")

    st.markdown("---")

    # Pexels ì„¤ì •
    st.subheader("Pexels ì„¤ì •")
    px_mode_label = st.selectbox("ê²°ê³¼ ìœ í˜•", ["ê²€ìƒ‰ ê²°ê³¼", "ì¸ê¸° ë™ì˜ìƒ"], index=0, key="sb_pxmode")
    px_mode = "search" if px_mode_label == "ê²€ìƒ‰ ê²°ê³¼" else "popular"

    st.markdown("---")

    # í…Œë§ˆ ìƒ‰ìƒ ì„ íƒ
    st.subheader("í¬ì¸íŠ¸ ìƒ‰ìƒ")
    st.session_state.accent = st.selectbox("ìƒ‰ìƒ", ["ê¸°ë³¸","ë¸”ë£¨","ê·¸ë¦°","í•‘í¬","ë³´ë¼"],
                                           index=["ê¸°ë³¸","ë¸”ë£¨","ê·¸ë¦°","í•‘í¬","ë³´ë¼"].index(st.session_state.accent),
                                           key="sb_accent")

# ===== ìƒë‹¨ íˆ´ë°”(ì •ë ¬/ì—´ìˆ˜) - ëª¨ë‘ key ë¶€ì—¬ =====
with st.container():
    tb1, tb2, tb3, tb4 = st.columns([1.3,1,1,1])
    with tb1:
        st.session_state.yt_sort = st.radio("YouTube ì •ë ¬", ["ì¡°íšŒìˆ˜ìˆœ","ìµœì‹ ìˆœ"],
                                            horizontal=True,
                                            index=["ì¡°íšŒìˆ˜ìˆœ","ìµœì‹ ìˆœ"].index(st.session_state.yt_sort),
                                            key="tb_sort")
    with tb2:
        st.session_state.left_cols = st.select_slider("YouTube ì—´ìˆ˜", options=[2,3],
                                                      value=st.session_state.left_cols, key="tb_left_cols")
    with tb3:
        st.session_state.right_cols = st.select_slider("Pexels ì—´ìˆ˜", options=[2,3],
                                                       value=st.session_state.right_cols, key="tb_right_cols")
    with tb4:
        st.write("")
        st.caption("ì •ë ¬ê³¼ ì—´ìˆ˜ëŠ” ìƒë‹¨ì—ì„œ ë¹ ë¥´ê²Œ ë°”ê¿”ìš” âœ¨")

# =========================
# ê²€ìƒ‰ ì‹¤í–‰
# =========================
yt_df_all = pd.DataFrame()
px_df_all = pd.DataFrame()

if run and query:
    # ìµœê·¼ ê²€ìƒ‰ì–´ ì €ì¥ (10ê°œ ìœ ì§€)
    if query not in st.session_state.search_history:
        st.session_state.search_history.append(query)
    if len(st.session_state.search_history) > 10:
        st.session_state.search_history = st.session_state.search_history[-10:]

    with st.spinner("ê²€ìƒ‰ ì¤‘â€¦"):
        # YouTube
        try:
            order_mode = "viewCount" if st.session_state.yt_sort == "ì¡°íšŒìˆ˜ìˆœ" else "date"
            yt_all = search_youtube(
                query,
                fetch_total=st.session_state.yt_fetch_limit,
                cc_only=yt_cc,
                upload_window=yt_upload_window,
                include_channels=include_channels,
                exclude_channels=exclude_channels,
                include_channel_ids=include_channel_ids,
                exclude_channel_ids=exclude_channel_ids,
                include_words=include_words,
                exclude_words=exclude_words,
                region_code=(region_code or None),
                relevance_lang=(relevance_lang or None),
                safe_mode=yt_safe,
                order_mode=order_mode,
                duration_param=duration_param,
                min_seconds=min_seconds,
                max_seconds=max_seconds,
            )
            yt_df_all = pd.DataFrame(yt_all)
        except Exception as e:
            st.error(f"YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        # Pexels (ì˜¤ë¥¸ìª½ íŒ¨ë„)
        try:
            if PEXELS_API_KEY:
                px_all = search_pexels(query, fetch=max(50, st.session_state.page_size), mode=px_mode)
                px_df_all = pd.DataFrame(px_all)
            else:
                px_df_all = pd.DataFrame()
        except Exception as e:
            st.error(f"Pexels ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    # í˜ì´ì§€ë„¤ì´ì…˜ (ìƒë‹¨)
    total_rows = max(len(yt_df_all), len(px_df_all))
    page_controls(total_rows, where="top")

    yt_df_page = slice_df_for_page(yt_df_all) if not yt_df_all.empty else yt_df_all
    px_df_page = slice_df_for_page(px_df_all) if not px_df_all.empty else px_df_all

    # ===== ë ˆì´ì•„ì›ƒ ë Œë”ë§ =====
    def render_two_panels():
        left, sep, right = st.columns([1, 0.03, 1], gap="large")
        with left:
            st.subheader("ğŸ¬ YouTube")
            if yt_df_page.empty:
                st.info("YouTube ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                tab_all, tab_shorts, tab_video = st.tabs(["ì „ì²´","ì‡¼ì¸ ","ì˜ìƒ"])
                with tab_all:
                    render_cards(yt_df_page, cols=st.session_state.left_cols,
                                 subtitles=["author","views","durationText","publishedAt","license"],
                                 bookmark_key_prefix="yt")
                with tab_shorts:
                    df_s = _filter_shorts(yt_df_page, True)
                    render_cards(df_s, cols=st.session_state.left_cols,
                                 subtitles=["author","views","durationText","publishedAt","license"],
                                 bookmark_key_prefix="yt_s")
                with tab_video:
                    df_v = _filter_shorts(yt_df_page, False)
                    render_cards(df_v, cols=st.session_state.left_cols,
                                 subtitles=["author","views","durationText","publishedAt","license"],
                                 bookmark_key_prefix="yt_v")
        with sep:
            st.markdown("<div class='v-sep'>&nbsp;</div>", unsafe_allow_html=True)
        with right:
            st.subheader("ğŸ“¹ Pexels")
            if PEXELS_API_KEY and not px_df_page.empty:
                render_cards(px_df_page, cols=st.session_state.right_cols,
                             subtitles=["author","duration"], bookmark_key_prefix="px")
            else:
                st.info("Pexels API í‚¤ê°€ ì—†ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                if query:
                    st.link_button("ğŸ”— Pexelsì—ì„œ ê²€ìƒ‰í•˜ê¸°", pexels_search_link(query), use_container_width=True)
                st.caption("â€» ëª©ë¡ì„ ì•±ì—ì„œ ë³´ë ¤ë©´ PEXELS_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    def render_stacked():
        st.subheader("ğŸ¬ YouTube")
        if yt_df_page.empty:
            st.info("YouTube ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            tab_all, tab_shorts, tab_video = st.tabs(["ì „ì²´","ì‡¼ì¸ ","ì˜ìƒ"])
            with tab_all:
                render_cards(yt_df_page, cols=3,
                             subtitles=["author","views","durationText","publishedAt","license"],
                             bookmark_key_prefix="yt")
            with tab_shorts:
                df_s = _filter_shorts(yt_df_page, True)
                render_cards(df_s, cols=3,
                             subtitles=["author","views","durationText","publishedAt","license"],
                             bookmark_key_prefix="yt_s")
            with tab_video:
                df_v = _filter_shorts(yt_df_page, False)
                render_cards(df_v, cols=3,
                             subtitles=["author","views","durationText","publishedAt","license"],
                             bookmark_key_prefix="yt_v")
        st.markdown("---")
        st.subheader("ğŸ“¹ Pexels")
        if PEXELS_API_KEY and not px_df_page.empty:
            render_cards(px_df_page, cols=3, subtitles=["author","duration"], bookmark_key_prefix="px")
        else:
            st.info("Pexels API í‚¤ê°€ ì—†ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            if query:
                st.link_button("ğŸ”— Pexelsì—ì„œ ê²€ìƒ‰í•˜ê¸°", pexels_search_link(query), use_container_width=True)
            st.caption("â€» ëª©ë¡ì„ ì•±ì—ì„œ ë³´ë ¤ë©´ PEXELS_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    if layout_mode == "ê°€ë¡œ 2íŒ¨ë„":
        render_two_panels()
    else:
        render_stacked()

    # í˜ì´ì§€ë„¤ì´ì…˜ (í•˜ë‹¨)
    page_controls(total_rows, where="bottom")

    st.markdown("---")

    # ë¶ë§ˆí¬ & ë‚´ë³´ë‚´ê¸°
    st.subheader("â­ ë¶ë§ˆí¬")
    if st.session_state.bookmarks:
        bm_df = pd.DataFrame(st.session_state.bookmarks)
        if "url" in bm_df.columns:
            bm_df = bm_df.drop_duplicates(subset=["url"], keep="first")
        cols_to_show = [c for c in ["platform","title","author","views","url","durationText","publishedAt","license","duration"] if c in bm_df.columns]
        st.dataframe(bm_df[cols_to_show].fillna(""), use_container_width=True, height=260)
        st.download_button(
            "ë¶ë§ˆí¬ CSV ë‹¤ìš´ë¡œë“œ",
            bm_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"minsuk_lab_bookmarks_{int(time.time())}.csv", mime="text/csv"
        )
    else:
        st.caption("ì•„ì§ ë¶ë§ˆí¬ê°€ ì—†ì–´ìš”. ì¹´ë“œì˜ â­ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì¶”ê°€í•˜ì„¸ìš”.")

# Footer legal
st.markdown("<p style='font-variant-caps: all-small-caps; letter-spacing: .03em; color: #777;'>ë³¸ ë„êµ¬ëŠ” ì›ë³¸ ë§í¬ë¡œë§Œ ì´ë™í•˜ë©° ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</p>", unsafe_allow_html=True)
